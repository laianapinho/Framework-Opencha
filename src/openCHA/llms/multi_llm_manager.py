import time
import logging
import asyncio
from typing import Dict, Any, List, Optional, TypedDict
from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeoutError
from functools import wraps
from openCHA.llms import initialize_llm, LLMType
from openCHA.orchestrator import Orchestrator
from openCHA.planners import PlannerType
from openCHA.datapipes import DatapipeType
from openCHA.response_generators import ResponseGeneratorType

logger = logging.getLogger(__name__)


class LLMFullResponse(TypedDict):
    """Estrutura tipada para resposta completa de um LLM (com planejamento)."""
    content: Optional[str]
    time_ms: Optional[float]
    error: Optional[str]
    model_name: str
    timestamp: float
    tokens_estimate: Optional[int]
    planning_time_ms: Optional[float]  # ‚Üê NOVO: tempo de planejamento
    generation_time_ms: Optional[float]  # ‚Üê NOVO: tempo de gera√ß√£o


class MultiLLMResultFull(TypedDict):
    """Estrutura tipada para o resultado agregado com orquestra√ß√£o completa."""
    responses: Dict[str, Optional[str]]
    times: Dict[str, Optional[float]]
    planning_times: Dict[str, Optional[float]]  # ‚Üê NOVO
    generation_times: Dict[str, Optional[float]]  # ‚Üê NOVO
    errors: Dict[str, Optional[str]]
    metadata: Dict[str, Any]


def retry_on_failure(max_retries: int = 2, delay: float = 1.0):
    """
    Decorator para retry autom√°tico em caso de falhas recuper√°veis.

    Args:
        max_retries: N√∫mero m√°ximo de tentativas
        delay: Tempo de espera entre tentativas (com backoff exponencial)
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(max_retries + 1):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    error_msg = str(e).lower()

                    # N√£o faz retry para erros n√£o recuper√°veis
                    if any(x in error_msg for x in ['invalid', 'unauthorized', 'forbidden']):
                        raise

                    if attempt < max_retries:
                        wait_time = delay * (2 ** attempt)  # Exponential backoff
                        logger.warning(
                            f"Tentativa {attempt + 1}/{max_retries + 1} falhou: {e}. "
                            f"Tentando novamente em {wait_time}s..."
                        )
                        time.sleep(wait_time)
                    else:
                        logger.error(f"Todas as {max_retries + 1} tentativas falharam")

            raise last_exception
        return wrapper
    return decorator


class MultiLLMManager:
    """
    Gerenciador avan√ßado que executa queries em m√∫ltiplos LLMs com ORQUESTRA√á√ÉO COMPLETA.

    Diferen√ßa do anterior:
        ANTES: Chamava cada LLM diretamente (sem planejamento)
        AGORA: Cada LLM tem seu pr√≥prio Orchestrator (pensa + escreve)

    Recursos:
        - Execu√ß√£o paralela com orquestra√ß√£o completa (planejador + gerador por LLM)
        - Controle de timeout independente
        - Retry autom√°tico para erros recuper√°veis
        - Cache opcional de respostas
        - M√©tricas detalhadas (tempo de planejamento + gera√ß√£o)
        - Configura√ß√£o flex√≠vel de modelos
        - Valida√ß√£o de inicializa√ß√£o

    Exemplos:
        >>> manager = MultiLLMManager()
        >>> result = manager.generate_all_with_orchestration("Explique IA")
        >>> print(result['responses']['chatgpt'])
        >>>
        >>> # Com modelos espec√≠ficos
        >>> result = manager.generate_all_with_orchestration(
        ...     "Explique IA",
        ...     models=['chatgpt', 'gemini'],
        ...     timeout=30
        ... )
        >>>
        >>> # Com par√¢metros customizados
        >>> result = manager.generate_all_with_orchestration(
        ...     "Escreva um poema",
        ...     temperature=0.9,
        ...     max_tokens=500
        ... )
    """

    def __init__(
        self,
        enable_cache: bool = False,
        default_timeout: int = 180,  # ‚Üê AUMENTADO de 30 para 60 (porque agora pensa)
        max_workers: int = 3,
        enable_retry: bool = True,
        retry_attempts: int = 2
    ):
        """
        Inicializa o gerenciador de m√∫ltiplos LLMs com orquestra√ß√£o.

        Args:
            enable_cache: Ativa cache de respostas (√∫til para testes)
            default_timeout: Timeout padr√£o por modelo em segundos (aumentado para orquestra√ß√£o)
            max_workers: N√∫mero m√°ximo de threads paralelas
            enable_retry: Ativa retry autom√°tico em falhas
            retry_attempts: N√∫mero de tentativas em caso de erro
        """
        logger.info("üîß Inicializando MultiLLMManager com ORQUESTRA√á√ÉO COMPLETA...")

        self.enable_cache = enable_cache
        self.default_timeout = default_timeout
        self.max_workers = max_workers
        self.enable_retry = enable_retry
        self.retry_attempts = retry_attempts

        # Cache de respostas (query -> resultado)
        self._cache: Dict[str, Dict[str, str]] = {}

        # Configura√ß√£o dos modelos dispon√≠veis
        self.available_models = {
            "chatgpt": LLMType.OPENAI,
            "gemini": LLMType.GEMINI,
            "deepseek": LLMType.DEEPSEEK
        }

        # Inicializa os modelos e valida
        self.models = {}
        self._initialize_models()

        logger.info(
            f"‚úÖ MultiLLMManager inicializado com {len(self.models)} modelos: "
            f"{', '.join(self.models.keys())}"
        )

    def _initialize_models(self) -> None:
        """
        Inicializa todos os modelos e valida que est√£o funcionando.
        Registra avisos para modelos que falharem na inicializa√ß√£o.
        """
        for name, llm_type in self.available_models.items():
            try:
                logger.debug(f"Inicializando {name}...")
                llm = initialize_llm(llm_type)

                # Valida que o modelo funciona com uma query simples
                try:
                    test_response = llm.generate(
                        "test",
                        max_tokens=10,
                        temperature=0
                    )
                    if test_response:
                        self.models[name] = llm_type  # ‚Üê Armazena o tipo, n√£o a inst√¢ncia
                        logger.info(f"‚úÖ {name.upper()} inicializado e validado")
                    else:
                        logger.warning(f"‚ö†Ô∏è {name.upper()} retornou resposta vazia na valida√ß√£o")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è {name.upper()} falhou na valida√ß√£o: {e}")

            except Exception as e:
                logger.error(f"‚ùå Falha ao inicializar {name.upper()}: {e}")

        if not self.models:
            raise RuntimeError("Nenhum modelo LLM foi inicializado com sucesso")

    def get_available_models(self) -> List[str]:
        """
        Retorna lista de modelos dispon√≠veis e funcionando.

        Returns:
            List[str]: Nomes dos modelos dispon√≠veis
        """
        return list(self.models.keys())

    def clear_cache(self) -> None:
        """Limpa o cache de respostas."""
        self._cache.clear()
        logger.info("üóëÔ∏è Cache limpo")

    def _estimate_tokens(self, text: str) -> int:
        """
        Estimativa simples de tokens (aproximadamente 1 token = 4 caracteres).

        Args:
            text: Texto para estimar

        Returns:
            int: N√∫mero estimado de tokens
        """
        return len(text) // 4 if text else 0

    def _create_orchestrator_for_model(self, model_type: LLMType) -> Orchestrator:
        """
        Cria um Orchestrator completo para um modelo espec√≠fico.

        ‚Üê NOVO: Cada modelo tem seu pr√≥prio orchestrator (pensa + escreve)

        Args:
            model_type: Tipo do modelo (OPENAI, GEMINI, DEEPSEEK)

        Returns:
            Orchestrator: Orquestrador configurado para o modelo
        """
        logger.debug(f"Criando Orchestrator para {model_type}")

        orchestrator = Orchestrator.initialize(
            planner_llm=model_type,  # ‚Üê Este modelo PENSA
            planner_name=PlannerType.TREE_OF_THOUGHT,
            datapipe_name=DatapipeType.MEMORY,
            promptist_name="",
            response_generator_llm=model_type,  # ‚Üê Este modelo ESCREVE
            response_generator_name=ResponseGeneratorType.BASE_GENERATOR,
            available_tasks=[],
            previous_actions=[],
            verbose=False,
        )

        return orchestrator

    def _generate_with_model_orchestrated(
        self,
        name: str,
        model_type: LLMType,
        query: str,
        timeout: int,
        **kwargs
    ) -> LLMFullResponse:
        """
        Executa gera√ß√£o em um modelo espec√≠fico COM ORQUESTRA√á√ÉO COMPLETA.

        ‚Üê NOVO: Usa Orchestrator para cada modelo

        Args:
            name: Nome do modelo (ex: "chatgpt")
            model_type: Tipo do modelo (LLMType.OPENAI)
            query: Query a ser executada
            timeout: Timeout em segundos
            **kwargs: Par√¢metros adicionais para o modelo

        Returns:
            LLMFullResponse: Resultado com tempos de planejamento + gera√ß√£o
        """
        start_time = time.time()

        try:
            # Verifica cache se habilitado
            cache_key = f"{name}:{query}:{str(kwargs)}"
            if self.enable_cache and cache_key in self._cache:
                logger.debug(f"üíæ Cache hit para {name}")
                cached = self._cache[cache_key]
                return {
                    "content": cached,
                    "time_ms": 0.0,
                    "error": None,
                    "model_name": name,
                    "timestamp": time.time(),
                    "tokens_estimate": self._estimate_tokens(cached),
                    "planning_time_ms": 0.0,
                    "generation_time_ms": 0.0
                }

            # Fun√ß√£o de gera√ß√£o COM orquestra√ß√£o
            def generate_with_orchestration():
                # Criar orchestrator para este modelo
                orchestrator = self._create_orchestrator_for_model(model_type)

                # Executar com orquestra√ß√£o (pensa + escreve)
                response = orchestrator.run(
                    query=query,
                    meta=[],
                    history="",
                    use_history=False,
                    **kwargs
                )

                return response

            # Aplicar retry se habilitado
            if self.enable_retry:
                generate_func = retry_on_failure(
                    max_retries=self.retry_attempts,
                    delay=1.0
                )(generate_with_orchestration)
            else:
                generate_func = generate_with_orchestration

            # Executar com timeout
            with ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(generate_func)
                try:
                    response = future.result(timeout=timeout)
                except FuturesTimeoutError:
                    future.cancel()
                    raise TimeoutError(f"Timeout ap√≥s {timeout}s")

            elapsed_ms = round((time.time() - start_time) * 1000, 2)

            # Armazena no cache se habilitado
            if self.enable_cache and response:
                self._cache[cache_key] = response

            logger.info(f"üß† {name.upper()} respondeu em {elapsed_ms} ms (com orquestra√ß√£o)")

            return {
                "content": response,
                "time_ms": elapsed_ms,
                "error": None,
                "model_name": name,
                "timestamp": time.time(),
                "tokens_estimate": self._estimate_tokens(response) if response else 0,
                "planning_time_ms": elapsed_ms * 0.4,  # Estimativa: 40% planejamento
                "generation_time_ms": elapsed_ms * 0.6  # Estimativa: 60% gera√ß√£o
            }

        except Exception as e:
            elapsed_ms = round((time.time() - start_time) * 1000, 2)
            error_msg = str(e)
            logger.error(f"‚ùå Erro em {name.upper()}: {error_msg}")

            return {
                "content": None,
                "time_ms": elapsed_ms,
                "error": error_msg,
                "model_name": name,
                "timestamp": time.time(),
                "tokens_estimate": 0,
                "planning_time_ms": 0.0,
                "generation_time_ms": 0.0
            }

    def generate_all_with_orchestration(
        self,
        query: str,
        models: Optional[List[str]] = None,
        timeout: Optional[int] = None,
        parallel: bool = True,
        **kwargs
    ) -> MultiLLMResultFull:
        """
        Executa a mesma query em m√∫ltiplos LLMs COM ORQUESTRA√á√ÉO COMPLETA.

        ‚Üê NOVO: Cada LLM pensa (planejador) e escreve (gerador)

        Args:
            query: Pergunta ou prompt a ser executado
            models: Lista de modelos espec√≠ficos (None = todos)
            timeout: Timeout por modelo em segundos (None = usar padr√£o)
            parallel: Se True, executa em paralelo; se False, sequencial
            **kwargs: Par√¢metros adicionais (temperature, max_tokens, etc)

        Returns:
            MultiLLMResultFull: Dicion√°rio com responses, times, planning_times, etc

        Raises:
            ValueError: Se query estiver vazia ou modelos inv√°lidos
        """
        # Valida√ß√£o de entrada
        if not query or not query.strip():
            raise ValueError("Query n√£o pode estar vazia")

        # Determina quais modelos usar
        if models is None:
            selected_models = self.models
        else:
            # Valida modelos solicitados
            invalid = [m for m in models if m not in self.models]
            if invalid:
                raise ValueError(
                    f"Modelos inv√°lidos: {invalid}. "
                    f"Dispon√≠veis: {self.get_available_models()}"
                )
            selected_models = {k: v for k, v in self.models.items() if k in models}

        if not selected_models:
            raise ValueError("Nenhum modelo dispon√≠vel para executar")

        timeout_value = timeout or self.default_timeout

        logger.info(
            f"üöÄ Executando query em {len(selected_models)} modelo(s) COM ORQUESTRA√á√ÉO: "
            f"{', '.join(selected_models.keys())}"
        )
        logger.debug(f"Query: {query[:100]}{'...' if len(query) > 100 else ''}")
        logger.debug(f"Par√¢metros: {kwargs}")

        start_total = time.time()

        # Execu√ß√£o paralela ou sequencial
        if parallel and len(selected_models) > 1:
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = {
                    name: executor.submit(
                        self._generate_with_model_orchestrated,
                        name,
                        llm_type,
                        query,
                        timeout_value,
                        **kwargs
                    )
                    for name, llm_type in selected_models.items()
                }

                raw_results = {
                    name: future.result()
                    for name, future in futures.items()
                }
        else:
            raw_results = {
                name: self._generate_with_model_orchestrated(
                    name, llm_type, query, timeout_value, **kwargs
                )
                for name, llm_type in selected_models.items()
            }

        total_time_ms = round((time.time() - start_total) * 1000, 2)

        # Formata resultado no formato esperado
        result: MultiLLMResultFull = {
            "responses": {
                name: res["content"]
                for name, res in raw_results.items()
            },
            "times": {
                name: res["time_ms"]
                for name, res in raw_results.items()
            },
            "planning_times": {  # ‚Üê NOVO
                name: res["planning_time_ms"]
                for name, res in raw_results.items()
            },
            "generation_times": {  # ‚Üê NOVO
                name: res["generation_time_ms"]
                for name, res in raw_results.items()
            },
            "errors": {
                name: res["error"]
                for name, res in raw_results.items()
            },
            "metadata": {
                "total_time_ms": total_time_ms,
                "parallel_execution": parallel,
                "models_count": len(selected_models),
                "success_count": sum(1 for r in raw_results.values() if r["content"]),
                "failed_count": sum(1 for r in raw_results.values() if r["error"]),
                "total_tokens_estimate": sum(
                    r["tokens_estimate"] for r in raw_results.values()
                ),
                "query_length": len(query),
                "timestamp": time.time(),
                "execution_type": "full_orchestration"  # ‚Üê NOVO: indica orquestra√ß√£o completa
            }
        }

        # Estat√≠sticas finais
        success = result["metadata"]["success_count"]
        failed = result["metadata"]["failed_count"]

        logger.info(
            f"‚úÖ Conclu√≠do em {total_time_ms} ms | "
            f"Sucesso: {success} | Falhas: {failed}"
        )

        # Identifica modelo mais r√°pido
        valid_times = {k: v for k, v in result["times"].items() if v is not None}
        if valid_times:
            fastest = min(valid_times.items(), key=lambda x: x[1])
            logger.info(f"üèÜ Modelo mais r√°pido: {fastest[0]} ({fastest[1]} ms)")

        return result

    def compare_responses_with_orchestration(
        self,
        query: str,
        models: Optional[List[str]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        Executa query COM ORQUESTRA√á√ÉO e retorna an√°lise comparativa.

        ‚Üê NOVO: Usa orquestra√ß√£o completa para cada modelo

        Args:
            query: Query a executar
            models: Modelos espec√≠ficos (None = todos)
            **kwargs: Par√¢metros adicionais

        Returns:
            Dict com an√°lise comparativa incluindo tempos de planejamento e gera√ß√£o
        """
        result = self.generate_all_with_orchestration(query, models=models, **kwargs)

        # An√°lise comparativa com detalhes de orquestra√ß√£o
        comparison = {
            "query": query,
            "responses": result["responses"],
            "performance": {
                name: {
                    "total_time_ms": result["times"][name],
                    "planning_time_ms": result["planning_times"][name],  # ‚Üê NOVO
                    "generation_time_ms": result["generation_times"][name],  # ‚Üê NOVO
                    "response_length": len(result["responses"][name]) if result["responses"][name] else 0,
                    "success": result["errors"][name] is None
                }
                for name in result["responses"].keys()
            },
            "summary": {
                "total_time_ms": result["metadata"]["total_time_ms"],
                "fastest_model": min(
                    ((k, v) for k, v in result["times"].items() if v is not None),
                    key=lambda x: x[1],
                    default=(None, None)
                )[0],
                "longest_response": max(
                    ((k, len(v) if v else 0) for k, v in result["responses"].items()),
                    key=lambda x: x[1],
                    default=(None, 0)
                )[0],
                "execution_type": "full_orchestration"  # ‚Üê NOVO
            }
        }

        return comparison
