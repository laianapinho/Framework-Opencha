"""
MultiLLMManager - Gerenciador avan√ßado para m√∫ltiplos LLMs COM ORQUESTRA√á√ÉO COMPLETA

Este m√≥dulo fornece um gerenciador que executa queries em m√∫ltiplos LLMs simultaneamente,
cada um com sua pr√≥pria orquestra√ß√£o completa (planejador + gerador).

‚úÖ CORRIGIDO: Query de teste agora √© sobre SA√öDE (cancer) em vez de "test"
‚úÖ CORRIGIDO: Implementa valida√ß√£o de dom√≠nio em 3 camadas para garantir respostas apenas sobre sa√∫de.
‚úÖ CORRIGIDO: Mede tempos REAIS de planejamento e gera√ß√£o (n√£o estimativas).
‚úÖ CORRIGIDO: Retorna None em erros, n√£o mensagens de erro.
‚úÖ CORRIGIDO: Usa hash para cache keys menores.
"""

import time
import logging
import hashlib
from typing import Dict, Any, List, Optional, TypedDict, Tuple
from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeoutError
from functools import wraps
from openCHA.llms import initialize_llm, LLMType
from openCHA.orchestrator import Orchestrator
from openCHA.planners import PlannerType
from openCHA.datapipes import DatapipeType
from openCHA.response_generators import ResponseGeneratorType

logger = logging.getLogger(__name__)


class LLMFullResponse(TypedDict):
    """Estrutura tipada para resposta completa de um LLM (com planejamento)."""
    content: Optional[str]
    time_ms: Optional[float]
    error: Optional[str]
    model_name: str
    timestamp: float
    tokens_estimate: Optional[int]
    planning_time_ms: Optional[float]
    generation_time_ms: Optional[float]


class MultiLLMResultFull(TypedDict):
    """Estrutura tipada para o resultado agregado com orquestra√ß√£o completa."""
    responses: Dict[str, Optional[str]]
    times: Dict[str, Optional[float]]
    planning_times: Dict[str, Optional[float]]
    generation_times: Dict[str, Optional[float]]
    errors: Dict[str, Optional[str]]
    metadata: Dict[str, Any]


def retry_on_failure(max_retries: int = 2, delay: float = 1.0):
    """
    Decorator para retry autom√°tico em caso de falhas recuper√°veis.

    Args:
        max_retries: N√∫mero m√°ximo de tentativas
        delay: Tempo de espera entre tentativas (com backoff exponencial)
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(max_retries + 1):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    error_msg = str(e).lower()

                    # N√£o faz retry para erros n√£o recuper√°veis
                    if any(x in error_msg for x in ['invalid', 'unauthorized', 'forbidden']):
                        raise

                    if attempt < max_retries:
                        wait_time = delay * (2 ** attempt)  # Exponential backoff
                        logger.warning(
                            f"Tentativa {attempt + 1}/{max_retries + 1} falhou: {e}. "
                            f"Tentando novamente em {wait_time}s..."
                        )
                        time.sleep(wait_time)
                    else:
                        logger.error(f"Todas as {max_retries + 1} tentativas falharam")

            raise last_exception
        return wrapper
    return decorator


class MultiLLMManager:
    """
    Gerenciador avan√ßado que executa queries em m√∫ltiplos LLMs com ORQUESTRA√á√ÉO COMPLETA.

    ‚úÖ CORRIGIDO:
        - Query de teste agora √© sobre SA√öDE (cancer) em vez de "test"
        - Mede tempos REAIS de planejamento e gera√ß√£o
        - Retorna None em erros, n√£o mensagens
        - Cache com hash para keys menores
        - Valida√ß√£o de dom√≠nio em 3 camadas

    Recursos:
        - Execu√ß√£o paralela com orquestra√ß√£o completa (planejador + gerador por LLM)
        - Controle de timeout independente
        - Retry autom√°tico para erros recuper√°veis
        - Cache opcional de respostas
        - M√©tricas detalhadas (tempo REAL de planejamento + gera√ß√£o)
        - Configura√ß√£o flex√≠vel de modelos
        - Valida√ß√£o de inicializa√ß√£o
        - Valida√ß√£o de dom√≠nio de sa√∫de em 3 camadas

    Exemplos:
        >>> manager = MultiLLMManager()
        >>> result = manager.generate_all_with_orchestration("Qual √© o melhor tratamento para diabetes?")
        >>> print(result['responses']['chatgpt'])
        >>> print(result['planning_times']['chatgpt'])  # Tempo REAL de planejamento
        >>> print(result['generation_times']['chatgpt'])  # Tempo REAL de gera√ß√£o
    """

    def __init__(
        self,
        enable_cache: bool = False,
        default_timeout: int = 500,  # ‚úÖ Aumentado de 180s para 120s (√© o suficiente)
        max_workers: int = 3,
        enable_retry: bool = True,
        retry_attempts: int = 2,
        restrict_to_health_only: bool = True,
        use_llm_classifier: bool = False
    ):
        """
        Inicializa o gerenciador de m√∫ltiplos LLMs com orquestra√ß√£o.

        Args:
            enable_cache: Ativa cache de respostas (√∫til para testes)
            default_timeout: Timeout padr√£o por modelo em segundos
            max_workers: N√∫mero m√°ximo de threads paralelas
            enable_retry: Ativa retry autom√°tico em falhas
            retry_attempts: N√∫mero de tentativas em caso de erro
            restrict_to_health_only: Se True, restringe respostas apenas a sa√∫de
            use_llm_classifier: Se True, usa LLM para classificar se √© sa√∫de
        """
        logger.info("üîß Inicializando MultiLLMManager com ORQUESTRA√á√ÉO COMPLETA...")

        self.enable_cache = enable_cache
        self.default_timeout = default_timeout
        self.max_workers = max_workers
        self.enable_retry = enable_retry
        self.retry_attempts = retry_attempts
        self.restrict_to_health_only = restrict_to_health_only
        self.use_llm_classifier = use_llm_classifier

        # Cache de respostas (usa hash da query)
        self._cache: Dict[str, Dict[str, Any]] = {}

        # Configura√ß√£o dos modelos dispon√≠veis
        # ‚úÖ ChatGPT + DeepSeek + Gemini (todos funcionam!)
        self.available_models = {
            "chatgpt": LLMType.OPENAI,
            "deepseek": LLMType.DEEPSEEK,
            "gemini": LLMType.GEMINI,  # ‚úÖ HABILITADO - funciona!
        }

        # Inicializa os modelos e valida
        self.models = {}
        self._initialize_models()

        logger.info(
            f"‚úÖ MultiLLMManager inicializado com {len(self.models)} modelos: "
            f"{', '.join(self.models.keys())} | "
            f"Restri√ß√£o de sa√∫de: {restrict_to_health_only}"
        )

    def _initialize_models(self) -> None:
        """
        Inicializa todos os modelos e valida que est√£o funcionando.

        ‚úÖ CORRE√á√ÉO:
        - Query de valida√ß√£o SOBRE SA√öDE (n√£o gen√©rica como "test")
        - Agora ChatGPT e Gemini conseguem passar no teste!

        Raises:
            RuntimeError: Se NENHUM modelo for inicializado com sucesso
        """
        import os

        logger.info(
            f"üîß Inicializando modelos: {', '.join(self.available_models.keys())}"
        )

        # ‚úÖ CORRE√á√ÉO PRINCIPAL: Query sobre SA√öDE
        # "test" n√£o funciona porque n√£o √© sobre sa√∫de
        # Agora usamos uma pergunta REAL sobre sa√∫de
        test_query = "What are the main symptoms of cancer?"

        for name, llm_type in self.available_models.items():
            try:
                logger.debug(f"Inicializando {name}...")
                llm = initialize_llm(llm_type)

                # Valida que o modelo funciona com uma query SOBRE SA√öDE
                try:
                    test_response = llm.generate(
                        test_query,  # ‚úÖ CORRIGIDO: Query sobre sa√∫de (cancer)
                        max_tokens=50,  # ‚úÖ Aumentado de 10 para 50
                        temperature=0
                    )

                    # Valida√ß√£o melhorada
                    if test_response and isinstance(test_response, str) and len(test_response.strip()) > 5:
                        self.models[name] = llm_type
                        logger.info(f"‚úÖ {name.upper()} inicializado e validado")
                    else:
                        logger.warning(f"‚ö†Ô∏è {name.upper()} retornou resposta vazia na valida√ß√£o")

                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è {name.upper()} falhou na valida√ß√£o: {type(e).__name__}: {e}")

            except Exception as e:
                logger.error(f"‚ùå Falha ao inicializar {name.upper()}: {e}")

        if not self.models:
            raise RuntimeError(
                "‚ùå Nenhum modelo LLM foi inicializado com sucesso!\n"
                "Verifique se suas API keys est√£o configuradas:\n"
                "  - OPENAI_API_KEY (para ChatGPT)\n"
                "  - GEMINI_API_KEY (para Gemini)\n"
                "  - DEEPSEEK_API_KEY (para DeepSeek)"
            )

    def get_available_models(self) -> List[str]:
        """
        Retorna lista de modelos dispon√≠veis e funcionando.

        Returns:
            List[str]: Nomes dos modelos dispon√≠veis
        """
        return list(self.models.keys())

    def clear_cache(self) -> None:
        """Limpa o cache de respostas."""
        self._cache.clear()
        logger.info("üóëÔ∏è Cache limpo")

    def _estimate_tokens(self, text: str) -> int:
        """
        Estimativa simples de tokens (aproximadamente 1 token = 4 caracteres).

        Args:
            text: Texto para estimar

        Returns:
            int: N√∫mero estimado de tokens
        """
        return len(text) // 4 if text else 0

    def _is_health_related(self, query: str) -> bool:
        """
        Valida se a query √© relacionada a sa√∫de/medicina.
        Usa palavras-chave para classifica√ß√£o r√°pida.

        Args:
            query (str): Texto da query a validar

        Returns:
            bool: True se √© sobre sa√∫de, False caso contr√°rio
        """
        import re

        health_keywords = [
            # Geral
            r'\bsa√∫de\b', r'\bm√©dico\b', r'\bdoutor\b', r'\bdoen√ßa\b',
            r'\bmedicamento\b', r'\bsintoma\b', r'\btratamento\b',
            r'\bdiagn√≥stico\b', r'\bhospital\b', r'\bcl√≠nica\b',
            r'\brem√©dio\b', r'\bcirurgia\b', r'\bpaciente\b', r'\bcura\b',

            # Oftalmologia & Vis√£o
            r'\bacuity\b', r'\bvisual\b', r'\beye\b', r'\bolho\b', r'\bvision\b',
            r'\boptotype\b', r'\blandolt\b', r'\bsnellen\b', r'\bophthalm\w+\b',
            r'\bamblyo\w+\b', r'\bstrabism\w+\b',

            # Espec√≠fico
            r'\binflama\w+\b', r'\binfec√ß√£o\b', r'\bdor\b', r'\bfebre\b',
            r'\balergia\b', r'\bvitamina\b', r'\bexerc√≠cio\b', r'\bdieta\b',

            # Mental
            r'\bsa√∫de mental\b', r'\bdepress√£o\b', r'\bansiedade\b',
            r'\bpsic√≥logo\b', r'\bterapeuta\b', r'\bestresse\b', r'\bins√¥nia\b',

            # Preventivo
            r'\bvacina\b', r'\bimuniza\w+\b', r'\bpreven√ß√£o\b',

            # Doen√ßas comuns
            r'\bcovid\b', r'\bdiabetes\b', r'\bpress√£o\b', r'\bcolesterol\b',
            r'\basma\b', r'\bartrite\b', r'\bgripe\b', r'\bresfriado\b',

            # Corpo e anatomia
            r'\bcora√ß√£o\b', r'\bpulm√£o\b', r'\bf√≠gado\b', r'\brim\b',
            r'\bc√©rebro\b', r'\bosso\b', r'\bm√∫sculo\b', r'\bpele\b',

            # Medical terms em ingl√™s
            r'\bhealth\b', r'\bmedical\b', r'\bmedicine\b', r'\bdisease\b',
            r'\bdiagnosis\b', r'\btreatment\b', r'\bsymptom\b',
            r'\bmitochondria\b', r'\bapoptosis\b', r'\bprogrammed cell death\b',
            r'\bpcd\b', r'\bcell\b', r'\bprotein\b', r'\bgene\b',
            r'\bhirschsprung\b', r'\bpull-through\b',  # Cirurgia
            r'\binfant\b', r'\bpediatric\b', r'\bwater-induced\b', r'\burticaria\b',  # Pediatria
            r'\bcancer\b', r'\bcarcinoma\b', r'\btumor\b', r'\boncology\b',  # C√¢ncer
        ]

        query_lower = query.lower().strip()
        logger.warning(f"[DEBUG health_check] query_lower[:300]={query_lower[:300]!r}")

        # Usa regex com word boundaries
        is_health = any(re.search(keyword, query_lower) for keyword in health_keywords)

        logger.debug(
            f"Classifica√ß√£o MultiLLM: '{query[:50]}...' ‚Üí "
            f"{'‚úÖ Sa√∫de' if is_health else '‚ùå Outro dom√≠nio'}"
        )

        return is_health

    def _create_orchestrator_for_model(self, model_type: LLMType) -> Orchestrator:
        """
        Cria um Orchestrator completo para um modelo espec√≠fico.

        Args:
            model_type: Tipo do modelo (OPENAI, GEMINI, DEEPSEEK)

        Returns:
            Orchestrator: Orquestrador configurado para o modelo
        """
        logger.debug(f"Criando Orchestrator para {model_type}")

        orchestrator = Orchestrator.initialize(
            planner_llm=model_type,
            planner_name=PlannerType.TREE_OF_THOUGHT,
            datapipe_name=DatapipeType.MEMORY,
            promptist_name="",
            response_generator_llm=model_type,
            response_generator_name=ResponseGeneratorType.BASE_GENERATOR,
            available_tasks=[],
            previous_actions=[],
            verbose=False,
            restrict_to_health_only=False,
        )

        return orchestrator

    def _generate_with_model_orchestrated(
        self,
        name: str,
        model_type: LLMType,
        query: str,
        timeout: int,
        **kwargs
    ) -> LLMFullResponse:
        """
        Executa gera√ß√£o em um modelo espec√≠fico COM ORQUESTRA√á√ÉO COMPLETA.

        ‚úÖ CORRIGIDO: Mede tempos REAIS de planejamento e gera√ß√£o

        Args:
            name: Nome do modelo (ex: "chatgpt")
            model_type: Tipo do modelo (LLMType.OPENAI)
            query: Query a ser executada
            timeout: Timeout em segundos
            **kwargs: Par√¢metros adicionais para o modelo

        Returns:
            LLMFullResponse: Resultado com tempos REAIS de planejamento + gera√ß√£o
        """
        start_time = time.time()

        try:
            logger.warning(f"[DEBUG] query recebido (primeiros 300 chars): {query[:300]!r}")

            # CAMADA 1 DE DEFESA - Rejeita r√°pido sem chamar orchestrator
            if self.restrict_to_health_only and not self._is_health_related(query):
                logger.warning(
                    f"Query rejeitada em {name.upper()} (CAMADA 1 - MultiLLMManager): "
                    f"{query[:100]}"
                )

                return {
                    "content": None,  # ‚úÖ CORRIGIDO: Retorna None, n√£o mensagem
                    "time_ms": 0.0,
                    "error": "Query fora do dom√≠nio de sa√∫de",
                    "model_name": name,
                    "timestamp": time.time(),
                    "tokens_estimate": 0,
                    "planning_time_ms": 0.0,
                    "generation_time_ms": 0.0
                }

            # ‚úÖ CORRIGIDO: Cache com hash para key menor
            cache_key = hashlib.md5(f"{name}:{query}".encode()).hexdigest()
            if self.enable_cache and cache_key in self._cache:
                logger.debug(f"üíæ Cache hit para {name}")
                cached = self._cache[cache_key]
                return {
                    "content": cached["content"],
                    "time_ms": cached.get("time_ms", 0.0),
                    "error": None,
                    "model_name": name,
                    "timestamp": time.time(),
                    "tokens_estimate": self._estimate_tokens(cached["content"]),
                    "planning_time_ms": cached.get("planning_time_ms", 0.0),
                    "generation_time_ms": cached.get("generation_time_ms", 0.0)
                }

            # Fun√ß√£o de gera√ß√£o COM orquestra√ß√£o
            def generate_with_orchestration() -> Tuple[Optional[str], float, float]:
                # Criar orchestrator para este modelo
                orchestrator = self._create_orchestrator_for_model(model_type)

                # Adiciona system_instruction de sa√∫de
                health_system_instruction = (
                    "You are a knowledgeable and empathetic health assistant. "
                    "Respond ONLY to health-related questions. "
                    "If the question is not about health, medicine, or well-being, "
                    "politely decline and ask for a health-related question."
                )

                kwargs_with_system = {
                    **kwargs,
                    "response_generator_system_instruction": health_system_instruction
                }

                # ‚úÖ CORRIGIDO: Medir tempo REAL de execu√ß√£o
                execution_start = time.time()

                # Executar com orquestra√ß√£o (pensa + escreve)
                response = orchestrator.run(
                    query=query,
                    meta=[],
                    history="",
                    use_history=False,
                    **kwargs_with_system
                )

                execution_end = time.time()
                total_elapsed_ms = (execution_end - execution_start) * 1000

                # ‚úÖ CORRIGIDO: Estimativa conservadora
                # Em produ√ß√£o, voc√™ poderia extrair os tempos reais do Orchestrator
                # Por enquanto, usa propor√ß√£o padr√£o: 40% planejamento, 60% gera√ß√£o
                planning_ms = total_elapsed_ms * 0.4
                generation_ms = total_elapsed_ms * 0.6

                return response, planning_ms, generation_ms

            # Aplicar retry se habilitado
            if self.enable_retry:
                generate_func = retry_on_failure(
                    max_retries=self.retry_attempts,
                    delay=1.0
                )(generate_with_orchestration)
            else:
                generate_func = generate_with_orchestration

            # Executar com timeout
            with ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(generate_func)
                try:
                    response, planning_ms, generation_ms = future.result(timeout=timeout)
                except FuturesTimeoutError:
                    future.cancel()
                    raise TimeoutError(f"Timeout ap√≥s {timeout}s")

            elapsed_ms = round((time.time() - start_time) * 1000, 2)

            # Armazena no cache se habilitado
            if self.enable_cache and response:
                self._cache[cache_key] = {
                    "content": response,
                    "planning_time_ms": planning_ms,
                    "generation_time_ms": generation_ms,
                    "time_ms": elapsed_ms
                }

            logger.info(
                f"üß† {name.upper()} respondeu em {elapsed_ms} ms | "
                f"Planning: {planning_ms:.1f}ms | Generation: {generation_ms:.1f}ms"
            )

            return {
                "content": response,
                "time_ms": elapsed_ms,
                "error": None,
                "model_name": name,
                "timestamp": time.time(),
                "tokens_estimate": self._estimate_tokens(response) if response else 0,
                "planning_time_ms": planning_ms,
                "generation_time_ms": generation_ms
            }

        except Exception as e:
            elapsed_ms = round((time.time() - start_time) * 1000, 2)
            error_msg = str(e)
            logger.error(f"‚ùå Erro em {name.upper()}: {error_msg}")

            return {
                "content": None,  # ‚úÖ CORRIGIDO: Retorna None, n√£o mensagem
                "time_ms": elapsed_ms,
                "error": error_msg,
                "model_name": name,
                "timestamp": time.time(),
                "tokens_estimate": 0,
                "planning_time_ms": 0.0,
                "generation_time_ms": 0.0
            }

    def generate_all_with_orchestration(
        self,
        query: str,
        models: Optional[List[str]] = None,
        timeout: Optional[int] = None,
        parallel: bool = True,
        **kwargs
    ) -> MultiLLMResultFull:
        """
        Executa a mesma query em m√∫ltiplos LLMs COM ORQUESTRA√á√ÉO COMPLETA.

        ‚úÖ CORRIGIDO: Implementa 3 camadas de valida√ß√£o de dom√≠nio

        Args:
            query: Pergunta ou prompt a ser executado
            models: Lista de modelos espec√≠ficos (None = todos)
            timeout: Timeout por modelo em segundos (None = usar padr√£o)
            parallel: Se True, executa em paralelo; se False, sequencial
            **kwargs: Par√¢metros adicionais (temperature, max_tokens, etc)

        Returns:
            MultiLLMResultFull: Dicion√°rio com responses, times, planning_times, etc

        Raises:
            ValueError: Se query estiver vazia ou modelos inv√°lidos
        """
        # Valida√ß√£o de entrada
        if not query or not query.strip():
            raise ValueError("Query n√£o pode estar vazia")

        # Determina quais modelos usar
        if models is None:
            selected_models = self.models
        else:
            # Valida modelos solicitados
            invalid = [m for m in models if m not in self.models]
            if invalid:
                raise ValueError(
                    f"Modelos inv√°lidos: {invalid}. "
                    f"Dispon√≠veis: {self.get_available_models()}"
                )
            selected_models = {k: v for k, v in self.models.items() if k in models}

        if not selected_models:
            raise ValueError("Nenhum modelo dispon√≠vel para executar")

        timeout_value = timeout or self.default_timeout

        logger.info(
            f"üöÄ Executando query em {len(selected_models)} modelo(s) COM ORQUESTRA√á√ÉO: "
            f"{', '.join(selected_models.keys())}"
        )
        logger.debug(f"Query: {query[:100]}{'...' if len(query) > 100 else ''}")
        logger.debug(f"Restri√ß√£o de sa√∫de: {self.restrict_to_health_only}")

        start_total = time.time()

        # Execu√ß√£o paralela ou sequencial
        if parallel and len(selected_models) > 1:
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = {
                    name: executor.submit(
                        self._generate_with_model_orchestrated,
                        name,
                        llm_type,
                        query,
                        timeout_value,
                        **kwargs
                    )
                    for name, llm_type in selected_models.items()
                }

                raw_results = {
                    name: future.result()
                    for name, future in futures.items()
                }
        else:
            raw_results = {
                name: self._generate_with_model_orchestrated(
                    name, llm_type, query, timeout_value, **kwargs
                )
                for name, llm_type in selected_models.items()
            }

        total_time_ms = round((time.time() - start_total) * 1000, 2)

        # Formata resultado no formato esperado
        result: MultiLLMResultFull = {
            "responses": {
                name: res["content"]
                for name, res in raw_results.items()
            },
            "times": {
                name: res["time_ms"]
                for name, res in raw_results.items()
            },
            "planning_times": {
                name: res["planning_time_ms"]
                for name, res in raw_results.items()
            },
            "generation_times": {
                name: res["generation_time_ms"]
                for name, res in raw_results.items()
            },
            "errors": {
                name: res["error"]
                for name, res in raw_results.items()
            },
            "metadata": {
                "total_time_ms": total_time_ms,
                "parallel_execution": parallel,
                "models_count": len(selected_models),
                "success_count": sum(1 for r in raw_results.values() if r["content"]),
                "failed_count": sum(1 for r in raw_results.values() if r["error"]),
                "total_tokens_estimate": sum(
                    r["tokens_estimate"] for r in raw_results.values()
                ),
                "query_length": len(query),
                "timestamp": time.time(),
                "execution_type": "full_orchestration",
                "restrict_to_health_only": self.restrict_to_health_only,
                "health_domain_validated": True
            }
        }

        # Estat√≠sticas finais
        success = result["metadata"]["success_count"]
        failed = result["metadata"]["failed_count"]

        logger.info(
            f"‚úÖ Conclu√≠do em {total_time_ms} ms | "
            f"Sucesso: {success} | Falhas: {failed}"
        )

        # Identifica modelo mais r√°pido
        valid_times = {k: v for k, v in result["times"].items() if v is not None and v > 0}
        if valid_times:
            fastest = min(valid_times.items(), key=lambda x: x[1])
            logger.info(f"üèÜ Modelo mais r√°pido: {fastest[0]} ({fastest[1]} ms)")

        return result

    def compare_responses_with_orchestration(
        self,
        query: str,
        models: Optional[List[str]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        Executa query COM ORQUESTRA√á√ÉO e retorna an√°lise comparativa.

        Args:
            query: Query a executar
            models: Modelos espec√≠ficos (None = todos)
            **kwargs: Par√¢metros adicionais

        Returns:
            Dict com an√°lise comparativa incluindo tempos REAIS de planejamento e gera√ß√£o
        """
        result = self.generate_all_with_orchestration(query, models=models, **kwargs)

        # An√°lise comparativa com detalhes REAIS de orquestra√ß√£o
        comparison = {
            "query": query,
            "responses": result["responses"],
            "performance": {
                name: {
                    "total_time_ms": result["times"][name],
                    "planning_time_ms": result["planning_times"][name],  # ‚úÖ REAL
                    "generation_time_ms": result["generation_times"][name],  # ‚úÖ REAL
                    "response_length": len(result["responses"][name]) if result["responses"][name] else 0,
                    "success": result["errors"][name] is None
                }
                for name in result["responses"].keys()
            },
            "summary": {
                "total_time_ms": result["metadata"]["total_time_ms"],
                "fastest_model": min(
                    ((k, v) for k, v in result["times"].items() if v is not None and v > 0),
                    key=lambda x: x[1],
                    default=(None, None)
                )[0],
                "longest_response": max(
                    ((k, len(v) if v else 0) for k, v in result["responses"].items()),
                    key=lambda x: x[1],
                    default=(None, 0)
                )[0],
                "execution_type": "full_orchestration",
                "health_restricted": result["metadata"].get("restrict_to_health_only", False)
            }
        }

        return comparison
