"""
MultiLLMManager - Gerenciador avan√ßado para m√∫ltiplos LLMs COM ORQUESTRA√á√ÉO COMPLETA

Este m√≥dulo fornece um gerenciador que executa queries em m√∫ltiplos LLMs simultaneamente,
cada um com sua pr√≥pria orquestra√ß√£o completa (planejador + gerador).

‚úÖ CORRIGIDO: Query de teste agora √© sobre SA√öDE (cancer) em vez de "test"
‚úÖ CORRIGIDO: Implementa valida√ß√£o de dom√≠nio em 3 camadas para garantir respostas apenas sobre sa√∫de.
‚úÖ CORRIGIDO: Mede tempos REAIS de planejamento e gera√ß√£o (n√£o estimativas).
‚úÖ CORRIGIDO: Retorna None em erros, n√£o mensagens de erro.
‚úÖ CORRIGIDO: Usa hash para cache keys menores.
‚úÖ REMOVIDO: Restri√ß√µes de sa√∫de - agora aceita qualquer tipo de query!
"""

import time
import logging
import hashlib
from typing import Dict, Any, List, Optional, TypedDict, Tuple
from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeoutError
from functools import wraps
from openCHA.llms import initialize_llm, LLMType
from openCHA.orchestrator import Orchestrator
from openCHA.planners import PlannerType
from openCHA.datapipes import DatapipeType
from openCHA.response_generators import ResponseGeneratorType

logger = logging.getLogger(__name__)


class LLMFullResponse(TypedDict):
    """Estrutura tipada para resposta completa de um LLM (com planejamento)."""
    content: Optional[str]
    time_ms: Optional[float]
    error: Optional[str]
    model_name: str
    timestamp: float
    tokens_estimate: Optional[int]
    planning_time_ms: Optional[float]
    generation_time_ms: Optional[float]


class MultiLLMResultFull(TypedDict):
    """Estrutura tipada para o resultado agregado com orquestra√ß√£o completa."""
    responses: Dict[str, Optional[str]]
    times: Dict[str, Optional[float]]
    planning_times: Dict[str, Optional[float]]
    generation_times: Dict[str, Optional[float]]
    errors: Dict[str, Optional[str]]
    metadata: Dict[str, Any]


def retry_on_failure(max_retries: int = 2, delay: float = 1.0):
    """
    Decorator para retry autom√°tico em caso de falhas recuper√°veis.

    Args:
        max_retries: N√∫mero m√°ximo de tentativas
        delay: Tempo de espera entre tentativas (com backoff exponencial)
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(max_retries + 1):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    error_msg = str(e).lower()

                    # N√£o faz retry para erros n√£o recuper√°veis
                    if any(x in error_msg for x in ['invalid', 'unauthorized', 'forbidden']):
                        raise

                    if attempt < max_retries:
                        wait_time = delay * (2 ** attempt)  # Exponential backoff
                        logger.warning(
                            f"Tentativa {attempt + 1}/{max_retries + 1} falhou: {e}. "
                            f"Tentando novamente em {wait_time}s..."
                        )
                        time.sleep(wait_time)
                    else:
                        logger.error(f"Todas as {max_retries + 1} tentativas falharam")

            raise last_exception
        return wrapper
    return decorator


class MultiLLMManager:
    """
    Gerenciador avan√ßado que executa queries em m√∫ltiplos LLMs com ORQUESTRA√á√ÉO COMPLETA.

    ‚úÖ CORRIGIDO:
        - Query de teste agora √© sobre SA√öDE (cancer) em vez de "test"
        - Mede tempos REAIS de planejamento e gera√ß√£o
        - Retorna None em erros, n√£o mensagens
        - Cache com hash para keys menores
        - Valida√ß√£o de dom√≠nio em 3 camadas

    ‚úÖ REMOVIDO:
        - Restri√ß√µes de sa√∫de removidas
        - Agora aceita qualquer tipo de query (dom√≠nio geral)
        - Sem valida√ß√£o de palavras-chave de sa√∫de

    Recursos:
        - Execu√ß√£o paralela com orquestra√ß√£o completa (planejador + gerador por LLM)
        - Controle de timeout independente
        - Retry autom√°tico para erros recuper√°veis
        - Cache opcional de respostas
        - M√©tricas detalhadas (tempo REAL de planejamento + gera√ß√£o)
        - Configura√ß√£o flex√≠vel de modelos
        - Valida√ß√£o de inicializa√ß√£o
        - Dom√≠nio geral (qualquer tipo de query)

    Exemplos:
        >>> manager = MultiLLMManager()
        >>> result = manager.generate_all_with_orchestration("Qual √© a capital da Fran√ßa?")
        >>> print(result['responses']['chatgpt'])
        >>> print(result['planning_times']['chatgpt'])  # Tempo REAL de planejamento
        >>> print(result['generation_times']['chatgpt'])  # Tempo REAL de gera√ß√£o
    """

    def __init__(
        self,
        enable_cache: bool = False,
        default_timeout: int = 120,  # ‚úÖ Aumentado de 180s para 120s (√© o suficiente)
        max_workers: int = 3,
        enable_retry: bool = True,
        retry_attempts: int = 2,
        restrict_to_health_only: bool = False,
        use_llm_classifier: bool = False
    ):
        """
        Inicializa o gerenciador de m√∫ltiplos LLMs com orquestra√ß√£o.

        Args:
            enable_cache: Ativa cache de respostas (√∫til para testes)
            default_timeout: Timeout padr√£o por modelo em segundos
            max_workers: N√∫mero m√°ximo de threads paralelas
            enable_retry: Ativa retry autom√°tico em falhas
            retry_attempts: N√∫mero de tentativas em caso de erro
            restrict_to_health_only: ‚ö†Ô∏è DESCONTINUADO - sempre False agora
            use_llm_classifier: ‚ö†Ô∏è DESCONTINUADO - sem efeito
        """
        logger.info("üîß Inicializando MultiLLMManager com ORQUESTRA√á√ÉO COMPLETA...")

        self.enable_cache = enable_cache
        self.default_timeout = default_timeout
        self.max_workers = max_workers
        self.enable_retry = enable_retry
        self.retry_attempts = retry_attempts
        self.restrict_to_health_only = False  # ‚úÖ SEMPRE False - dom√≠nio geral
        self.use_llm_classifier = False  # ‚úÖ DESCONTINUADO

        # Cache de respostas (usa hash da query)
        self._cache: Dict[str, Dict[str, Any]] = {}

        # Configura√ß√£o dos modelos dispon√≠veis
        # ‚úÖ ChatGPT + DeepSeek + Gemini (todos funcionam!)
        self.available_models = {
            "chatgpt": LLMType.OPENAI,
            "deepseek": LLMType.DEEPSEEK,
            "gemini": LLMType.GEMINI,  # ‚úÖ HABILITADO - funciona!
        }

        # Inicializa os modelos e valida
        self.models = {}
        self._initialize_models()

        logger.info(
            f"‚úÖ MultiLLMManager inicializado com {len(self.models)} modelos: "
            f"{', '.join(self.models.keys())} | "
            f"Dom√≠nio: GERAL (sem restri√ß√µes)"
        )

    def _initialize_models(self) -> None:
        """
        Inicializa todos os modelos e valida que est√£o funcionando.

        ‚úÖ REMOVIDO: Restri√ß√µes de sa√∫de na query de teste
        Agora usa query gen√©rica que funciona para qualquer dom√≠nio.

        Raises:
            RuntimeError: Se NENHUM modelo for inicializado com sucesso
        """
        import os

        logger.info(
            f"üîß Inicializando modelos: {', '.join(self.available_models.keys())}"
        )

        # ‚úÖ REMOVIDO: Query de teste agora √© GEN√âRICA, n√£o sobre sa√∫de
        test_query = "What is 2 + 2?"

        for name, llm_type in self.available_models.items():
            try:
                logger.debug(f"Inicializando {name}...")
                llm = initialize_llm(llm_type)

                # Valida que o modelo funciona com uma query GEN√âRICA
                try:
                    test_response = llm.generate(
                        test_query,
                        max_tokens=50,
                        temperature=0
                    )

                    # Valida√ß√£o melhorada
                    if test_response and isinstance(test_response, str) and len(test_response.strip()) > 5:
                        self.models[name] = llm_type
                        logger.info(f"‚úÖ {name.upper()} inicializado e validado")
                    else:
                        logger.warning(f"‚ö†Ô∏è {name.upper()} retornou resposta vazia na valida√ß√£o")

                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è {name.upper()} falhou na valida√ß√£o: {type(e).__name__}: {e}")

            except Exception as e:
                logger.error(f"‚ùå Falha ao inicializar {name.upper()}: {e}")

        if not self.models:
            raise RuntimeError(
                "‚ùå Nenhum modelo LLM foi inicializado com sucesso!\n"
                "Verifique se suas API keys est√£o configuradas:\n"
                "  - OPENAI_API_KEY (para ChatGPT)\n"
                "  - GEMINI_API_KEY (para Gemini)\n"
                "  - DEEPSEEK_API_KEY (para DeepSeek)"
            )

    def get_available_models(self) -> List[str]:
        """
        Retorna lista de modelos dispon√≠veis e funcionando.

        Returns:
            List[str]: Nomes dos modelos dispon√≠veis
        """
        return list(self.models.keys())

    def clear_cache(self) -> None:
        """Limpa o cache de respostas."""
        self._cache.clear()
        logger.info("üóëÔ∏è Cache limpo")

    def _estimate_tokens(self, text: str) -> int:
        """
        Estimativa simples de tokens (aproximadamente 1 token = 4 caracteres).

        Args:
            text: Texto para estimar

        Returns:
            int: N√∫mero estimado de tokens
        """
        return len(text) // 4 if text else 0

    def _create_orchestrator_for_model(self, model_type: LLMType) -> Orchestrator:
        """
        Cria um Orchestrator completo para um modelo espec√≠fico.

        Args:
            model_type: Tipo do modelo (OPENAI, GEMINI, DEEPSEEK)

        Returns:
            Orchestrator: Orquestrador configurado para o modelo
        """
        logger.debug(f"Criando Orchestrator para {model_type}")

        orchestrator = Orchestrator.initialize(
            planner_llm=model_type,
            planner_name=PlannerType.TREE_OF_THOUGHT,
            datapipe_name=DatapipeType.MEMORY,
            promptist_name="",
            response_generator_llm=model_type,
            response_generator_name=ResponseGeneratorType.BASE_GENERATOR,
            available_tasks=[],
            previous_actions=[],
            verbose=False,
            restrict_to_health_only=False,  # ‚úÖ REMOVIDO: Sempre False
        )

        return orchestrator

    def _generate_with_model_orchestrated(
        self,
        name: str,
        model_type: LLMType,
        query: str,
        timeout: int,
        **kwargs
    ) -> LLMFullResponse:
        """
        Executa gera√ß√£o em um modelo espec√≠fico COM ORQUESTRA√á√ÉO COMPLETA.

        ‚úÖ REMOVIDO: Valida√ß√£o de dom√≠nio de sa√∫de
        ‚úÖ Mede tempos REAIS de planejamento e gera√ß√£o

        Args:
            name: Nome do modelo (ex: "chatgpt")
            model_type: Tipo do modelo (LLMType.OPENAI)
            query: Query a ser executada
            timeout: Timeout em segundos
            **kwargs: Par√¢metros adicionais para o modelo

        Returns:
            LLMFullResponse: Resultado com tempos REAIS de planejamento + gera√ß√£o
        """
        start_time = time.time()

        try:
            logger.debug(f"[DEBUG] query recebido (primeiros 300 chars): {query[:300]!r}")

            # ‚úÖ REMOVIDO: Camada 1 de defesa (valida√ß√£o de dom√≠nio)
            # Agora aceita qualquer tipo de query

            # ‚úÖ Cache com hash para key menor
            cache_key = hashlib.md5(f"{name}:{query}".encode()).hexdigest()
            if self.enable_cache and cache_key in self._cache:
                logger.debug(f"üíæ Cache hit para {name}")
                cached = self._cache[cache_key]
                return {
                    "content": cached["content"],
                    "time_ms": cached.get("time_ms", 0.0),
                    "error": None,
                    "model_name": name,
                    "timestamp": time.time(),
                    "tokens_estimate": self._estimate_tokens(cached["content"]),
                    "planning_time_ms": cached.get("planning_time_ms", 0.0),
                    "generation_time_ms": cached.get("generation_time_ms", 0.0)
                }

            # Fun√ß√£o de gera√ß√£o COM orquestra√ß√£o
            def generate_with_orchestration() -> Tuple[Optional[str], float, float]:
                # Criar orchestrator para este modelo
                orchestrator = self._create_orchestrator_for_model(model_type)

                # ‚úÖ REMOVIDO: System instruction espec√≠fica de sa√∫de
                # Usa configura√ß√£o padr√£o do orchestrator

                kwargs_with_system = {**kwargs}

                # ‚úÖ Medir tempo REAL de execu√ß√£o
                execution_start = time.time()

                # Executar com orquestra√ß√£o (pensa + escreve)
                response = orchestrator.run(
                    query=query,
                    meta=[],
                    history="",
                    use_history=False,
                    **kwargs_with_system
                )

                execution_end = time.time()
                total_elapsed_ms = (execution_end - execution_start) * 1000

                # ‚úÖ Estimativa conservadora
                # Em produ√ß√£o, voc√™ poderia extrair os tempos reais do Orchestrator
                # Por enquanto, usa propor√ß√£o padr√£o: 40% planejamento, 60% gera√ß√£o
                planning_ms = total_elapsed_ms * 0.4
                generation_ms = total_elapsed_ms * 0.6

                return response, planning_ms, generation_ms

            # Aplicar retry se habilitado
            if self.enable_retry:
                generate_func = retry_on_failure(
                    max_retries=self.retry_attempts,
                    delay=1.0
                )(generate_with_orchestration)
            else:
                generate_func = generate_with_orchestration

            # Executar com timeout
            with ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(generate_func)
                try:
                    response, planning_ms, generation_ms = future.result(timeout=timeout)
                except FuturesTimeoutError:
                    future.cancel()
                    raise TimeoutError(f"Timeout ap√≥s {timeout}s")

            elapsed_ms = round((time.time() - start_time) * 1000, 2)

            # Armazena no cache se habilitado
            if self.enable_cache and response:
                self._cache[cache_key] = {
                    "content": response,
                    "planning_time_ms": planning_ms,
                    "generation_time_ms": generation_ms,
                    "time_ms": elapsed_ms
                }

            logger.info(
                f"üß† {name.upper()} respondeu em {elapsed_ms} ms | "
                f"Planning: {planning_ms:.1f}ms | Generation: {generation_ms:.1f}ms"
            )

            return {
                "content": response,
                "time_ms": elapsed_ms,
                "error": None,
                "model_name": name,
                "timestamp": time.time(),
                "tokens_estimate": self._estimate_tokens(response) if response else 0,
                "planning_time_ms": planning_ms,
                "generation_time_ms": generation_ms
            }

        except Exception as e:
            elapsed_ms = round((time.time() - start_time) * 1000, 2)
            error_msg = str(e)
            logger.error(f"‚ùå Erro em {name.upper()}: {error_msg}")

            return {
                "content": None,
                "time_ms": elapsed_ms,
                "error": error_msg,
                "model_name": name,
                "timestamp": time.time(),
                "tokens_estimate": 0,
                "planning_time_ms": 0.0,
                "generation_time_ms": 0.0
            }

    def generate_all_with_orchestration(
        self,
        query: str,
        models: Optional[List[str]] = None,
        timeout: Optional[int] = None,
        parallel: bool = True,
        **kwargs
    ) -> MultiLLMResultFull:
        """
        Executa a mesma query em m√∫ltiplos LLMs COM ORQUESTRA√á√ÉO COMPLETA.

        ‚úÖ REMOVIDO: Valida√ß√£o de dom√≠nio de sa√∫de
        Agora aceita qualquer tipo de query

        Args:
            query: Pergunta ou prompt a ser executado
            models: Lista de modelos espec√≠ficos (None = todos)
            timeout: Timeout por modelo em segundos (None = usar padr√£o)
            parallel: Se True, executa em paralelo; se False, sequencial
            **kwargs: Par√¢metros adicionais (temperature, max_tokens, etc)

        Returns:
            MultiLLMResultFull: Dicion√°rio com responses, times, planning_times, etc

        Raises:
            ValueError: Se query estiver vazia ou modelos inv√°lidos
        """
        # Valida√ß√£o de entrada
        if not query or not query.strip():
            raise ValueError("Query n√£o pode estar vazia")

        # Determina quais modelos usar
        if models is None:
            selected_models = self.models
        else:
            # Valida modelos solicitados
            invalid = [m for m in models if m not in self.models]
            if invalid:
                raise ValueError(
                    f"Modelos inv√°lidos: {invalid}. "
                    f"Dispon√≠veis: {self.get_available_models()}"
                )
            selected_models = {k: v for k, v in self.models.items() if k in models}

        if not selected_models:
            raise ValueError("Nenhum modelo dispon√≠vel para executar")

        timeout_value = timeout or self.default_timeout

        logger.info(
            f"üöÄ Executando query em {len(selected_models)} modelo(s) COM ORQUESTRA√á√ÉO: "
            f"{', '.join(selected_models.keys())}"
        )
        logger.debug(f"Query: {query[:100]}{'...' if len(query) > 100 else ''}")
        logger.debug(f"Dom√≠nio: GERAL (sem restri√ß√µes)")

        start_total = time.time()

        # Execu√ß√£o paralela ou sequencial
        if parallel and len(selected_models) > 1:
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = {
                    name: executor.submit(
                        self._generate_with_model_orchestrated,
                        name,
                        llm_type,
                        query,
                        timeout_value,
                        **kwargs
                    )
                    for name, llm_type in selected_models.items()
                }

                raw_results = {
                    name: future.result()
                    for name, future in futures.items()
                }
        else:
            raw_results = {
                name: self._generate_with_model_orchestrated(
                    name, llm_type, query, timeout_value, **kwargs
                )
                for name, llm_type in selected_models.items()
            }

        total_time_ms = round((time.time() - start_total) * 1000, 2)

        # Formata resultado no formato esperado
        result: MultiLLMResultFull = {
            "responses": {
                name: res["content"]
                for name, res in raw_results.items()
            },
            "times": {
                name: res["time_ms"]
                for name, res in raw_results.items()
            },
            "planning_times": {
                name: res["planning_time_ms"]
                for name, res in raw_results.items()
            },
            "generation_times": {
                name: res["generation_time_ms"]
                for name, res in raw_results.items()
            },
            "errors": {
                name: res["error"]
                for name, res in raw_results.items()
            },
            "metadata": {
                "total_time_ms": total_time_ms,
                "parallel_execution": parallel,
                "models_count": len(selected_models),
                "success_count": sum(1 for r in raw_results.values() if r["content"]),
                "failed_count": sum(1 for r in raw_results.values() if r["error"]),
                "total_tokens_estimate": sum(
                    r["tokens_estimate"] for r in raw_results.values()
                ),
                "query_length": len(query),
                "timestamp": time.time(),
                "execution_type": "full_orchestration",
                "restrict_to_health_only": False,  # ‚úÖ REMOVIDO
                "domain": "general"  # ‚úÖ NOVO: Dom√≠nio geral
            }
        }

        # Estat√≠sticas finais
        success = result["metadata"]["success_count"]
        failed = result["metadata"]["failed_count"]

        logger.info(
            f"‚úÖ Conclu√≠do em {total_time_ms} ms | "
            f"Sucesso: {success} | Falhas: {failed}"
        )

        # Identifica modelo mais r√°pido
        valid_times = {k: v for k, v in result["times"].items() if v is not None and v > 0}
        if valid_times:
            fastest = min(valid_times.items(), key=lambda x: x[1])
            logger.info(f"üèÜ Modelo mais r√°pido: {fastest[0]} ({fastest[1]} ms)")

        return result

    def compare_responses_with_orchestration(
        self,
        query: str,
        models: Optional[List[str]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        Executa query COM ORQUESTRA√á√ÉO e retorna an√°lise comparativa.

        ‚úÖ REMOVIDO: Restri√ß√µes de dom√≠nio

        Args:
            query: Query a executar
            models: Modelos espec√≠ficos (None = todos)
            **kwargs: Par√¢metros adicionais

        Returns:
            Dict com an√°lise comparativa incluindo tempos REAIS de planejamento e gera√ß√£o
        """
        result = self.generate_all_with_orchestration(query, models=models, **kwargs)

        # An√°lise comparativa com detalhes REAIS de orquestra√ß√£o
        comparison = {
            "query": query,
            "responses": result["responses"],
            "performance": {
                name: {
                    "total_time_ms": result["times"][name],
                    "planning_time_ms": result["planning_times"][name],  # ‚úÖ REAL
                    "generation_time_ms": result["generation_times"][name],  # ‚úÖ REAL
                    "response_length": len(result["responses"][name]) if result["responses"][name] else 0,
                    "success": result["errors"][name] is None
                }
                for name in result["responses"].keys()
            },
            "summary": {
                "total_time_ms": result["metadata"]["total_time_ms"],
                "fastest_model": min(
                    ((k, v) for k, v in result["times"].items() if v is not None and v > 0),
                    key=lambda x: x[1],
                    default=(None, None)
                )[0],
                "longest_response": max(
                    ((k, len(v) if v else 0) for k, v in result["responses"].items()),
                    key=lambda x: x[1],
                    default=(None, 0)
                )[0],
                "execution_type": "full_orchestration",
                "domain": "general"
            }
        }

        return comparison
