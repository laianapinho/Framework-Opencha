import time
import logging
import hashlib
from typing import Dict, Any, List, Optional, TypedDict, Tuple
from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeoutError
from functools import wraps

from openCHA.llms import initialize_llm, LLMType
from openCHA.orchestrator import Orchestrator
from openCHA.planners import PlannerType
from openCHA.datapipes import DatapipeType
from openCHA.response_generators import ResponseGeneratorType

logger = logging.getLogger(__name__)


class LLMFullResponse(TypedDict):
    content: Optional[str]
    time_ms: Optional[float]
    error: Optional[str]
    model_name: str
    timestamp: float
    tokens_estimate: Optional[int]
    planning_time_ms: Optional[float]
    generation_time_ms: Optional[float]


class MultiLLMResultFull(TypedDict):
    responses: Dict[str, Optional[str]]
    times: Dict[str, Optional[float]]
    planning_times: Dict[str, Optional[float]]
    generation_times: Dict[str, Optional[float]]
    errors: Dict[str, Optional[str]]
    metadata: Dict[str, Any]


def retry_on_failure(max_retries: int = 2, delay: float = 1.0):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(max_retries + 1):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    error_msg = str(e).lower()

                    if any(x in error_msg for x in ["invalid", "unauthorized", "forbidden"]):
                        raise

                    if attempt < max_retries:
                        wait_time = delay * (2 ** attempt)
                        logger.warning(
                            f"Tentativa {attempt + 1}/{max_retries + 1} falhou: {e}. "
                            f"Tentando novamente em {wait_time}s..."
                        )
                        time.sleep(wait_time)
                    else:
                        logger.error(f"Todas as {max_retries + 1} tentativas falharam")

            raise last_exception
        return wrapper
    return decorator


class MultiLLMManager:
    def __init__(
        self,
        enable_cache: bool = False,
        default_timeout: int = 120,
        max_workers: int = 3,
        enable_retry: bool = True,
        retry_attempts: int = 2,
        restrict_to_health_only: bool = False,
        use_llm_classifier: bool = False
    ):
        logger.info("ðŸ”§ Inicializando MultiLLMManager com ORQUESTRAÃ‡ÃƒO COMPLETA...")

        self.enable_cache = enable_cache
        self.default_timeout = default_timeout
        self.max_workers = max_workers
        self.enable_retry = enable_retry
        self.retry_attempts = retry_attempts

        self.restrict_to_health_only = False
        self.use_llm_classifier = False

        self._cache: Dict[str, Dict[str, Any]] = {}

        self.available_models = {
            "chatgpt": LLMType.OPENAI,
            "deepseek": LLMType.DEEPSEEK,
            "gemini": LLMType.GEMINI,
        }

        self.models: Dict[str, LLMType] = {}
        self._initialize_models()

        logger.info(
            f"âœ… MultiLLMManager inicializado com {len(self.models)} modelos: "
            f"{', '.join(self.models.keys())} | DomÃ­nio: GERAL"
        )

    def _initialize_models(self) -> None:
        logger.info(f"ðŸ”§ Inicializando modelos: {', '.join(self.available_models.keys())}")

        test_query = "What is 2 + 2?"

        for name, llm_type in self.available_models.items():
            try:
                llm = initialize_llm(llm_type)
                test_response = llm.generate(test_query, max_tokens=50, temperature=0)

                if test_response and isinstance(test_response, str) and len(test_response.strip()) > 0:
                    self.models[name] = llm_type
                    logger.info(f"âœ… {name.upper()} inicializado e validado")
                else:
                    logger.warning(f"âš ï¸ {name.upper()} retornou resposta vazia na validaÃ§Ã£o")

            except Exception as e:
                logger.warning(f"âš ï¸ {name.upper()} falhou na validaÃ§Ã£o: {type(e).__name__}: {e}")

        if not self.models:
            raise RuntimeError(
                "âŒ Nenhum modelo LLM foi inicializado com sucesso!\n"
                "Verifique suas API keys."
            )

    def get_available_models(self) -> List[str]:
        return list(self.models.keys())

    def clear_cache(self) -> None:
        self._cache.clear()
        logger.info("ðŸ—‘ï¸ Cache limpo")

    def _estimate_tokens(self, text: str) -> int:
        return len(text) // 4 if text else 0

    def _create_orchestrator_for_model(self, model_type: LLMType) -> Orchestrator:
        orchestrator = Orchestrator.initialize(
            planner_llm=model_type,
            planner_name=PlannerType.TREE_OF_THOUGHT,
            datapipe_name=DatapipeType.MEMORY,
            promptist_name="",
            response_generator_llm=model_type,
            response_generator_name=ResponseGeneratorType.BASE_GENERATOR,
            available_tasks=[],
            previous_actions=[],
            verbose=False,
            restrict_to_health_only=False,
        )
        return orchestrator

    def _generate_with_model_orchestrated(
        self,
        name: str,
        model_type: LLMType,
        query: str,
        timeout: int,
        **kwargs
    ) -> LLMFullResponse:
        start_time = time.time()

        try:
            cache_key = hashlib.md5(f"{name}:{query}".encode()).hexdigest()
            if self.enable_cache and cache_key in self._cache:
                cached = self._cache[cache_key]
                return {
                    "content": cached["content"],
                    "time_ms": cached.get("time_ms", 0.0),
                    "error": None,
                    "model_name": name,
                    "timestamp": time.time(),
                    "tokens_estimate": self._estimate_tokens(cached["content"]),
                    "planning_time_ms": cached.get("planning_time_ms", 0.0),
                    "generation_time_ms": cached.get("generation_time_ms", 0.0),
                }

            def generate_with_orchestration() -> Tuple[Optional[str], float, float]:
                orchestrator = self._create_orchestrator_for_model(model_type)

                # âœ… ALTERAÃ‡ÃƒO: Orchestrator devolve tempos REAIS
                response, timings = orchestrator.run(
                    query=query,
                    meta=[],
                    history="",
                    use_history=False,
                    return_timings=True,   # âœ… ALTERAÃ‡ÃƒO
                    **kwargs
                )

                planning_ms = float(timings.get("planning_time_ms", 0.0))
                generation_ms = float(timings.get("generation_time_ms", 0.0))
                return response, planning_ms, generation_ms

            if self.enable_retry:
                generate_func = retry_on_failure(max_retries=self.retry_attempts, delay=1.0)(generate_with_orchestration)
            else:
                generate_func = generate_with_orchestration

            with ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(generate_func)
                try:
                    response, planning_ms, generation_ms = future.result(timeout=timeout)
                except FuturesTimeoutError:
                    future.cancel()
                    raise TimeoutError(f"Timeout apÃ³s {timeout}s")

            elapsed_ms = round((time.time() - start_time) * 1000, 2)

            if self.enable_cache and response:
                self._cache[cache_key] = {
                    "content": response,
                    "planning_time_ms": planning_ms,
                    "generation_time_ms": generation_ms,
                    "time_ms": elapsed_ms,
                }

            logger.info(
                f"ðŸ§  {name.upper()} respondeu em {elapsed_ms} ms | "
                f"Planning (REAL): {planning_ms:.1f}ms | Generation (REAL): {generation_ms:.1f}ms"
            )

            return {
                "content": response,
                "time_ms": elapsed_ms,
                "error": None,
                "model_name": name,
                "timestamp": time.time(),
                "tokens_estimate": self._estimate_tokens(response) if response else 0,
                "planning_time_ms": planning_ms,
                "generation_time_ms": generation_ms,
            }

        except Exception as e:
            elapsed_ms = round((time.time() - start_time) * 1000, 2)
            error_msg = str(e)
            logger.error(f"âŒ Erro em {name.upper()}: {error_msg}")

            return {
                "content": None,
                "time_ms": elapsed_ms,
                "error": error_msg,
                "model_name": name,
                "timestamp": time.time(),
                "tokens_estimate": 0,
                "planning_time_ms": 0.0,
                "generation_time_ms": 0.0,
            }

    def generate_all_with_orchestration(
        self,
        query: str,
        models: Optional[List[str]] = None,
        timeout: Optional[int] = None,
        parallel: bool = True,
        **kwargs
    ) -> MultiLLMResultFull:
        if not query or not query.strip():
            raise ValueError("Query nÃ£o pode estar vazia")

        if models is None:
            selected_models = self.models
        else:
            invalid = [m for m in models if m not in self.models]
            if invalid:
                raise ValueError(f"Modelos invÃ¡lidos: {invalid}. DisponÃ­veis: {self.get_available_models()}")
            selected_models = {k: v for k, v in self.models.items() if k in models}

        if not selected_models:
            raise ValueError("Nenhum modelo disponÃ­vel para executar")

        timeout_value = timeout or self.default_timeout
        start_total = time.time()

        if parallel and len(selected_models) > 1:
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = {
                    name: executor.submit(
                        self._generate_with_model_orchestrated,
                        name,
                        llm_type,
                        query,
                        timeout_value,
                        **kwargs
                    )
                    for name, llm_type in selected_models.items()
                }
                raw_results = {name: future.result() for name, future in futures.items()}
        else:
            raw_results = {
                name: self._generate_with_model_orchestrated(name, llm_type, query, timeout_value, **kwargs)
                for name, llm_type in selected_models.items()
            }

        total_time_ms = round((time.time() - start_total) * 1000, 2)

        result: MultiLLMResultFull = {
            "responses": {name: res["content"] for name, res in raw_results.items()},
            "times": {name: res["time_ms"] for name, res in raw_results.items()},
            "planning_times": {name: res["planning_time_ms"] for name, res in raw_results.items()},
            "generation_times": {name: res["generation_time_ms"] for name, res in raw_results.items()},
            "errors": {name: res["error"] for name, res in raw_results.items()},
            "metadata": {
                "total_time_ms": total_time_ms,
                "parallel_execution": parallel,
                "models_count": len(selected_models),
                "success_count": sum(1 for r in raw_results.values() if r["content"]),
                "failed_count": sum(1 for r in raw_results.values() if r["error"]),
                "total_tokens_estimate": sum(r["tokens_estimate"] for r in raw_results.values()),
                "query_length": len(query),
                "timestamp": time.time(),
                "execution_type": "full_orchestration",
                "domain": "general",
            }
        }

        return result

    def compare_responses_with_orchestration(
        self,
        query: str,
        models: Optional[List[str]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        result = self.generate_all_with_orchestration(query, models=models, **kwargs)

        comparison = {
            "query": query,
            "responses": result["responses"],
            "performance": {
                name: {
                    "total_time_ms": result["times"][name],
                    "planning_time_ms": result["planning_times"][name],
                    "generation_time_ms": result["generation_times"][name],
                    "response_length": len(result["responses"][name]) if result["responses"][name] else 0,
                    "success": result["errors"][name] is None
                }
                for name in result["responses"].keys()
            },
            "summary": {
                "total_time_ms": result["metadata"]["total_time_ms"],
                "fastest_model": min(
                    ((k, v) for k, v in result["times"].items() if v is not None and v > 0),
                    key=lambda x: x[1],
                    default=(None, None)
                )[0],
                "execution_type": "full_orchestration",
                "domain": "general"
            }
        }

        return comparison
