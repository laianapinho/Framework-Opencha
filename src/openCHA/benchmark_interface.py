import gradio as gr
import logging
import time
import re
from typing import Callable
from openCHA.pubmedqa_loader import PubMedQALoader
from openCHA.benchmark_evaluator import BenchmarkEvaluator

logger = logging.getLogger(__name__)

class BenchmarkInterface:
    def __init__(self):
        self.loader = PubMedQALoader()
        self.evaluator = BenchmarkEvaluator()

    def extract_model_response(self, full_response: str, model_name: str) -> str:
        """
        Extrai a resposta de um modelo espec√≠fico do relat√≥rio Multi-LLM.

        Args:
            full_response: Resposta completa formatada Multi-LLM
            model_name: Nome do modelo (chatgpt, gemini, deepseek)

        Returns:
            str: Resposta do modelo ou vazio se n√£o encontrar
        """
        try:
            # Busca a se√ß√£o do modelo (ex: "CHATGPT")
            model_upper = model_name.upper()

            # Padr√£o: busca "ü§ñ MODELO\n" at√© a pr√≥xima se√ß√£o com "=" ou fim
            pattern = rf"ü§ñ\s+{model_upper}.*?üìù\s+Resposta:(.*?)(?:üèÜ|={10,}|$)"
            match = re.search(pattern, full_response, re.DOTALL | re.IGNORECASE)

            if match:
                resposta = match.group(1).strip()
                return resposta

            # Fallback: busca apenas a se√ß√£o com "MODEL:"
            if f"{model_upper}" in full_response:
                sections = full_response.split(f"{model_upper}")
                if len(sections) > 1:
                    # Pega texto at√© a pr√≥xima linha com "="
                    section = sections[1].split("=" * 80)[0]
                    # Remove linhas de timing
                    lines = [l for l in section.split('\n') if not l.strip().startswith(('‚è±Ô∏è', '‚îú', '‚îî', 'üìù'))]
                    resposta = '\n'.join(lines).strip()
                    if resposta:
                        return resposta

            return ""

        except Exception as e:
            logger.error(f"Erro ao extrair resposta de {model_name}: {e}")
            return ""

    def prepare_benchmark_tab(self, run_single_question: Callable, reset_fn: Callable):
        """
        Args:
            run_single_question: Fun√ß√£o do openCHA que roda pergunta com orquestra√ß√£o
                                Pode rodar em modo Multi-LLM para paralelo
            reset_fn: Fun√ß√£o de reset
        """
        with gr.Column():
            gr.Markdown("# üìä Benchmark PubMedQA - 3 Quest√µes (Paralelo)")

            models_to_test = gr.CheckboxGroup(
                label="Modelos",
                choices=["chatgpt", "gemini", "deepseek"],
                value=["chatgpt", "gemini", "deepseek"]
            )

            btn_start = gr.Button("üöÄ Iniciar Benchmark Paralelo", variant="primary")
            progress = gr.Textbox(label="Progresso", interactive=False, lines=3)
            result_text = gr.Textbox(label="Resultados", interactive=False, lines=40)

            def run_benchmark(models):
                results_text = "üìñ Carregando quest√µes...\n"
                questions = self.loader.get_subset(3)

                all_results = {model: {"correct": 0, "total": 0} for model in models}

                for i, q in enumerate(questions, 1):
                    results_text += f"\n{'='*80}\n"
                    results_text += f"‚ùì QUEST√ÉO {i}\n"
                    results_text += f"{'='*80}\n"
                    results_text += f"Pergunta: {q['question']}\n"
                    results_text += f"Resposta esperada: {q['expected_answer'].upper()}\n"
                    results_text += f"{'-'*80}\n"

                    try:
                        # ‚úÖ Chama Multi-LLM PARALELO
                        start = time.time()

                        full_response = run_single_question(
                            q['question'],
                            use_multi_llm=True,
                            compare_models=models
                        )

                        time_ms = (time.time() - start) * 1000

                        # Para cada modelo, extrai e avalia resposta
                        for model in models:
                            try:
                                # ‚úÖ EXTRAI resposta do modelo
                                model_response = self.extract_model_response(full_response, model)

                                # ‚úÖ AVALIA resposta
                                eval_result = self.evaluator.evaluate(
                                    q['expected_answer'],
                                    model_response
                                )

                                icon = "‚úÖ" if eval_result["correct"] else "‚ùå"

                                # ‚úÖ MOSTRA: modelo + resposta extra√≠da + resultado
                                results_text += f"\n{icon} {model.upper()}\n"
                                results_text += f"   Resposta: {model_response[:200]}\n"
                                results_text += f"   Detectado: {eval_result['extracted']} ({time_ms/len(models):.0f}ms)\n"

                                all_results[model]["total"] += 1
                                if eval_result["correct"]:
                                    all_results[model]["correct"] += 1

                            except Exception as e:
                                results_text += f"\n‚ùå {model.upper()}\n"
                                results_text += f"   Erro: {str(e)}\n"
                                logger.error(f"Erro ao processar {model}: {e}")

                    except Exception as e:
                        results_text += f"\n‚ùå Erro na quest√£o {i}: {str(e)}\n"
                        logger.error(f"Erro na quest√£o {i}: {e}")

                # ‚úÖ RESUMO FINAL
                results_text += f"\n{'='*80}\n"
                results_text += "üèÜ RESUMO FINAL (PARALELO):\n"
                results_text += f"{'='*80}\n"

                for model in models:
                    total = all_results[model]["total"]
                    if total > 0:
                        acc = all_results[model]["correct"] / total
                        results_text += f"{model.upper()}: {acc:.0%} ({all_results[model]['correct']}/{total})\n"
                    else:
                        results_text += f"{model.upper()}: Nenhuma quest√£o processada\n"

                return results_text, results_text

            btn_start.click(
                fn=run_benchmark,
                inputs=[models_to_test],
                outputs=[progress, result_text]
            )
