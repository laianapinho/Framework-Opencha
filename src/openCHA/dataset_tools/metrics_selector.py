"""
Metrics Selector - Seleciona m√©tricas apropriadas baseado no tipo de dataset
"""
import logging
from typing import Dict, List, Any, Callable
from enum import Enum

import numpy as np
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report
)

logger = logging.getLogger(__name__)

# Para m√©tricas de texto
try:
    from rouge_score import rouge_scorer
    ROUGE_AVAILABLE = True
except ImportError:
    ROUGE_AVAILABLE = False
    print("‚ö†Ô∏è  rouge-score n√£o instalado. Instale com: pip install rouge-score")

try:
    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
    BLEU_AVAILABLE = True
except ImportError:
    BLEU_AVAILABLE = False
    print("‚ö†Ô∏è  nltk n√£o instalado. Instale com: pip install nltk")

try:
    from sentence_transformers import SentenceTransformer
    EMBEDDING_AVAILABLE = True
except ImportError:
    EMBEDDING_AVAILABLE = False
    print("‚ö†Ô∏è  sentence-transformers n√£o instalado. Instale com: pip install sentence-transformers")

try:
    from nltk.translate.meteor_score import meteor_score
    import nltk
    # Tenta baixar WordNet automaticamente
    try:
        nltk.data.find('corpora/wordnet')
    except LookupError:
        logger.info("Baixando WordNet...")
        nltk.download('wordnet', quiet=True)
    METEOR_AVAILABLE = True
except ImportError:
    METEOR_AVAILABLE = False
    logger.warning("METEOR n√£o dispon√≠vel")

try:
    from bert_score import score as bert_score
    BERTSCORE_AVAILABLE = True
except ImportError:
    BERTSCORE_AVAILABLE = False
    print("‚ö†Ô∏è  bert-score n√£o instalado. Instale com: pip install bert-score")


class DatasetType(Enum):
    """Tipos de dataset"""
    CLOSED = "closed"  # Classifica√ß√£o (yes/no/maybe, labels, etc)
    OPEN = "open"  # Gera√ß√£o (respostas em texto livre)


class MetricsSelector:
    """Seleciona e calcula m√©tricas apropriadas para o tipo de dataset"""

    def __init__(self):
        self.embedding_model = None
        self._embedding_model_name = "sentence-transformers/all-MiniLM-L6-v2"

    def get_metrics_for_type(self, dataset_type: str) -> Dict[str, str]:
        """
        Retorna lista de m√©tricas recomendadas para o tipo de dataset.

        Args:
            dataset_type: 'closed' ou 'open'

        Returns:
            Dict com m√©tricas dispon√≠veis
        """
        if dataset_type == DatasetType.CLOSED.value or dataset_type == 'closed':
            return {
                'accuracy': 'Acur√°cia - propor√ß√£o de predi√ß√µes corretas',
                'precision': 'Precis√£o - propor√ß√£o de predi√ß√µes positivas corretas',
                'recall': 'Recall - propor√ß√£o de positivos identificados',
                'f1': 'F1-Score - m√©dia harm√¥nica entre precis√£o e recall',
                'confusion_matrix': 'Matriz de confus√£o - distribui√ß√£o de predi√ß√µes'
            }
        else:  # open
            return {
                'bleu': 'BLEU - Precis√£o de n-gramas (1-4)',
                'rouge_l': 'ROUGE-L - Longest common subsequence F-score',
                'semantic_similarity': 'Similaridade sem√¢ntica - embedding-based',
            }

    def calculate_closed_metrics(
        self,
        y_true: List[str],
        y_pred: List[str]
    ) -> Dict[str, Any]:
        """
        Calcula m√©tricas para dataset fechado (classifica√ß√£o).

        Args:
            y_true: Lista de r√≥tulos esperados
            y_pred: Lista de r√≥tulos preditos

        Returns:
            Dict com todas as m√©tricas
        """
        try:
            # Normaliza strings
            y_true_norm = [str(y).strip().lower() for y in y_true]
            y_pred_norm = [str(y).strip().lower() for y in y_pred]

            metrics = {
                'accuracy': accuracy_score(y_true_norm, y_pred_norm),
                'precision': precision_score(y_true_norm, y_pred_norm, average='weighted', zero_division=0),
                'recall': recall_score(y_true_norm, y_pred_norm, average='weighted', zero_division=0),
                'f1': f1_score(y_true_norm, y_pred_norm, average='weighted', zero_division=0),
            }

            # Tenta adicionar confusion matrix e classification report
            try:
                unique_labels = sorted(set(y_true_norm) | set(y_pred_norm))
                metrics['confusion_matrix'] = confusion_matrix(
                    y_true_norm, y_pred_norm, labels=unique_labels
                ).tolist()
                metrics['labels'] = unique_labels

                metrics['classification_report'] = classification_report(
                    y_true_norm, y_pred_norm, zero_division=0, output_dict=True
                )
            except Exception as e:
                logger.warning(f"Erro ao calcular confusion matrix: {e}")

            logger.info(f"‚úÖ M√©tricas de classifica√ß√£o calculadas")
            return metrics

        except Exception as e:
            logger.error(f"‚ùå Erro ao calcular m√©tricas de classifica√ß√£o: {e}")
            raise

    def calculate_open_metrics(
        self,
        references: List[str],
        predictions: List[str]
    ) -> Dict[str, Any]:
        """
        Calcula m√©tricas para dataset aberto (gera√ß√£o de texto).

        Args:
            references: Lista de respostas esperadas (gold standard)
            predictions: Lista de predi√ß√µes dos modelos

        Returns:
            Dict com todas as m√©tricas
        """
        logger.info(f"üîç DEBUG: Iniciando calculate_open_metrics com {len(references)} refs e {len(predictions)} preds")

        if len(references) != len(predictions):
            raise ValueError("Refer√™ncias e predi√ß√µes devem ter o mesmo tamanho")

        metrics = {
            'count': len(references),
            'individual_scores': []
        }

        logger.info(f"üîç DEBUG: BLEU_AVAILABLE={BLEU_AVAILABLE}, METEOR_AVAILABLE={METEOR_AVAILABLE}, BERTSCORE_AVAILABLE={BERTSCORE_AVAILABLE}")

        # ============================================================================
        # M√âTRICA 1: BLEU (Bilingual Evaluation Understudy)
        # ============================================================================
        # O que √©: Avalia a qualidade da gera√ß√£o de texto comparando n-gramas
        #          (sequ√™ncias de 1, 2, 3, 4 palavras) entre a predi√ß√£o e refer√™ncia
        #
        # Como funciona:
        # - Quebra texto em tokens (palavras)
        # - Compara n-gramas de tamanhos 1-4 entre predi√ß√£o e refer√™ncia
        # - Score de 0 a 1 (1 = match perfeito)
        # - Usa smoothing para evitar score 0 quando n√£o h√° matches
        #
        # Vantagem: R√°pido, usado em tradu√ß√£o autom√°tica
        # Desvantagem: N√£o captura sem√¢ntica, pode penalizar sin√¥nimos
        # ============================================================================
        if BLEU_AVAILABLE:
            bleu_scores = []
            for ref, pred in zip(references, predictions):
                try:
                    # Converte para min√∫sculas para compara√ß√£o uniforme
                    ref_tokens = ref.lower().split()
                    pred_tokens = pred.lower().split()

                    # Calcula BLEU com pesos iguais para 1-4 gramas
                    # weights=(0.25, 0.25, 0.25, 0.25) significa:
                    # - 25% baseado em palavras individuais (1-gramas)
                    # - 25% baseado em pares de palavras (2-gramas)
                    # - 25% baseado em triplas de palavras (3-gramas)
                    # - 25% baseado em quadruplas de palavras (4-gramas)
                    smoothing = SmoothingFunction().method1
                    bleu = sentence_bleu(
                        [ref_tokens],  # Refer√™ncia em lista (pode ter m√∫ltiplas)
                        pred_tokens,   # Predi√ß√£o a avaliar
                        weights=(0.25, 0.25, 0.25, 0.25),  # Pesos iguais para n-gramas
                        smoothing_function=smoothing  # Evita score 0
                    )
                    bleu_scores.append(bleu)
                except Exception as e:
                    logger.warning(f"Erro ao calcular BLEU: {e}")
                    bleu_scores.append(0.0)

            # M√©dia de todos os BLEUs calculados
            metrics['bleu'] = np.mean(bleu_scores) if bleu_scores else 0.0
            metrics['bleu_scores_individual'] = bleu_scores
        else:
            metrics['bleu'] = None
            logger.warning("BLEU n√£o dispon√≠vel (instale rouge-score)")

        # ============================================================================
        # M√âTRICA 2B: METEOR (Metric for Evaluation of Translation with Explicit Ordering)
        # ============================================================================
        # O que √©: Avalia qualidade de gera√ß√£o de texto considerando sin√¥nimos e par√°frases
        #          Desenvolvido para tradu√ß√£o autom√°tica mas funciona bem para QA tamb√©m
        #
        # Como funciona:
        # - Encontra matches de palavras entre predi√ß√£o e refer√™ncia
        # - Tipos de match:
        #   1. EXACT: Palavra id√™ntica ("syndrome" = "syndrome")
        #   2. STEM: Mesma raiz ("syndromes" = "syndrome" por stemming)
        #   3. SYNONYM: Sin√¥nimo ("illness" = "disease" por WordNet)
        # - Calcula Precision: matches / len(prediction)
        # - Calcula Recall: matches / len(reference)
        # - Aplica penalidade por falta de ordem ("fragile" vs "fragile X")
        # - Score de 0 a 1 (1 = match perfeito com ordem correta)
        #
        # Vantagem: Tolera sin√¥nimos, varia√ß√µes morfol√≥gicas, diferentes ordens
        # Desvantagem: Depende de dicion√°rios (WordNet), mais lento que BLEU
        #
        # Exemplo:
        # Esperado: "Fragile X chromosome syndrome"
        # Predi√ß√£o: "Fragile X Syndrome"
        # METEOR: Alto score (mesmo sendo mais curto, palavras principais matcham)
        # ============================================================================
        if METEOR_AVAILABLE:
            meteor_scores = []
            meteor_failed = False
            for ref, pred in zip(references, predictions):
                try:
                    # Tokeniza AMBOS em listas de tokens
                    ref_tokens = ref.lower().split()
                    pred_tokens = pred.lower().split()

                    # Calcula METEOR
                    meteor = meteor_score([ref_tokens], pred_tokens)
                    meteor_scores.append(meteor)
                except LookupError:
                    # WordNet n√£o encontrado
                    logger.warning("WordNet n√£o encontrado para METEOR. Desabilitando m√©trica.")
                    meteor_failed = True
                    break
                except Exception as e:
                    logger.warning(f"Erro ao calcular METEOR: {e}")
                    meteor_scores.append(0.0)

            if meteor_failed:
                metrics['meteor'] = None
            elif meteor_scores:
                metrics['meteor'] = np.mean(meteor_scores)
                metrics['meteor_scores_individual'] = meteor_scores
                logger.info(f"METEOR calculado: {metrics['meteor']:.4f}")
            else:
                metrics['meteor'] = None
        else:
            metrics['meteor'] = None

        # ============================================================================
        # M√âTRICA 2C: BERTScore
        # ============================================================================
        # O que √©: Compara textos usando embeddings de BERT (Transformers)
        #          Captura sem√¢ntica profunda e contexto cient√≠fico/m√©dico
        #
        # Como funciona:
        # - Converte cada token em embedding de alta dimens√£o usando BERT
        # - Compara similaridade de cosseno token-a-token
        # - Calcula Precision: m√©dia de matches mais similares em predi√ß√£o
        # - Calcula Recall: m√©dia de matches mais similares em refer√™ncia
        # - F1-score: m√©dia harm√¥nica entre Precision e Recall
        # - Score de 0 a 1 (1 = match sem√¢ntico perfeito)
        #
        # Especialmente bom para:
        # - Textos cient√≠ficos/m√©dicos (BERT entende dom√≠nio)
        # - Par√°frases e sin√¥nimos contextuais
        # - Respostas com ordem diferente mas significado igual
        #
        # Exemplo:
        # Esperado: "Fragile X chromosome syndrome"
        # Predi√ß√£o: "The disease caused by FMR1 gene mutation is Fragile X"
        # BERTScore: Reconhece que "FMR1" e "Fragile X" est√£o relacionados
        # (BLEU/ROUGE n√£o conseguiriam)
        #
        # Vantagem: Captura sem√¢ntica profunda, ideal para biomedicina
        # Desvantagem: Mais lento, requer GPU (opcional)
        # ============================================================================
        if BERTSCORE_AVAILABLE:
            try:
                # Usa modelo BERT padr√£o (english)
                # Pode usar modelo espec√≠fico com lang="en"
                P, R, F1_scores = bert_score(predictions, references, lang="en", verbose=False)

                # P = Precision (quanto da predi√ß√£o est√° na refer√™ncia)
                # R = Recall (quanto da refer√™ncia est√° na predi√ß√£o)
                # F1 = m√©dia harm√¥nica entre P e R

                metrics['bertscore_precision'] = P.mean().item()
                metrics['bertscore_recall'] = R.mean().item()
                metrics['bertscore_f1'] = F1_scores.mean().item()
                metrics['bertscore_f1_scores_individual'] = F1_scores.tolist()

                logger.info(f"BERTScore calculado - F1: {metrics['bertscore_f1']:.4f}")
            except Exception as e:
                logger.warning(f"Erro ao calcular BERTScore: {e}")
                metrics['bertscore_precision'] = None
                metrics['bertscore_recall'] = None
                metrics['bertscore_f1'] = None
        else:
            metrics['bertscore_precision'] = None
            metrics['bertscore_recall'] = None
            metrics['bertscore_f1'] = None
            logger.warning("BERTScore n√£o dispon√≠vel (instale com: pip install bert-score)")

        # ============================================================================
        # M√âTRICA 3: Similaridade Sem√¢ntica (Embedding-based)
        # ============================================================================
        # O que √©: Compara o SIGNIFICADO (sem√¢ntica) entre textos usando embeddings
        #          em vez de apenas palavras/n-gramas
        #
        # Como funciona:
        # - Converte cada texto em um vetor num√©rico de alta dimens√£o (384 dimens√µes)
        #   que captura o significado sem√¢ntico
        # - Calcula similaridade de cosseno entre os dois vetores
        # - Score de 0 a 1 (1 = significado id√™ntico)
        # - Dois textos com palavras diferentes mas mesmo significado t√™m score alto
        #
        # Exemplo:
        # - "O gato √© preto" vs "Um felino negro" = alta similaridade (mesmo significado)
        # - "O gato √© preto" vs "A mulher √© alta" = baixa similaridade (significado diferente)
        #
        # Vantagem: Captura sem√¢ntica real, tolera sin√¥nimos e par√°frases
        # Desvantagem: Mais lento, requer mais poder computacional
        # ============================================================================
        if EMBEDDING_AVAILABLE:
            try:
                semantic_scores = self._calculate_semantic_similarity(references, predictions)
                metrics['semantic_similarity'] = np.mean(semantic_scores) if semantic_scores else 0.0
                metrics['semantic_similarity_scores_individual'] = semantic_scores
            except Exception as e:
                logger.warning(f"Erro ao calcular similaridade sem√¢ntica: {e}")
                metrics['semantic_similarity'] = None
        else:
            metrics['semantic_similarity'] = None
            logger.warning("Similaridade sem√¢ntica n√£o dispon√≠vel (instale sentence-transformers)")

        # ============================================================================
        # Compilar scores individuais de cada quest√£o
        # ============================================================================
        # Para cada quest√£o, cria um dicion√°rio com todas as m√©tricas
        # Permite an√°lise por quest√£o, n√£o apenas m√©dia geral
        for i in range(len(references)):
            score_dict = {'index': i}

            # Adiciona BLEU se dispon√≠vel
            if metrics.get('bleu') is not None and metrics.get('bleu_scores_individual'):
                score_dict['bleu'] = metrics['bleu_scores_individual'][i]

            # Adiciona ROUGE-L se dispon√≠vel
            if metrics.get('rouge_l') is not None and metrics.get('rouge_l_scores_individual'):
                score_dict['rouge_l'] = metrics['rouge_l_scores_individual'][i]

            # Adiciona METEOR se dispon√≠vel
            if metrics.get('meteor') is not None and metrics.get('meteor_scores_individual'):
                score_dict['meteor'] = metrics['meteor_scores_individual'][i]

            # Adiciona BERTScore se dispon√≠vel
            if metrics.get('bertscore_f1') is not None and metrics.get('bertscore_f1_scores_individual'):
                score_dict['bertscore_f1'] = metrics['bertscore_f1_scores_individual'][i]

            # Adiciona Similaridade Sem√¢ntica se dispon√≠vel
            if metrics.get('semantic_similarity') is not None and metrics.get('semantic_similarity_scores_individual'):
                score_dict['semantic_similarity'] = metrics['semantic_similarity_scores_individual'][i]

            metrics['individual_scores'].append(score_dict)

        logger.info(f"‚úÖ M√©tricas de gera√ß√£o calculadas")
        return metrics

    def _calculate_semantic_similarity(
        self,
        references: List[str],
        predictions: List[str]
    ) -> List[float]:
        """
        Calcula similaridade sem√¢ntica usando embeddings.

        Args:
            references: Lista de textos de refer√™ncia
            predictions: Lista de textos preditos

        Returns:
            Lista de scores (0-1)
        """
        try:
            if self.embedding_model is None:
                logger.info(f"Carregando modelo de embedding: {self._embedding_model_name}")
                self.embedding_model = SentenceTransformer(self._embedding_model_name)

            # Gera embeddings
            ref_embeddings = self.embedding_model.encode(references, convert_to_tensor=False)
            pred_embeddings = self.embedding_model.encode(predictions, convert_to_tensor=False)

            # Calcula similaridade de cosseno
            from sklearn.metrics.pairwise import cosine_similarity

            scores = []
            for ref_emb, pred_emb in zip(ref_embeddings, pred_embeddings):
                sim = cosine_similarity([ref_emb], [pred_emb])[0][0]
                scores.append(float(sim))

            return scores

        except Exception as e:
            logger.error(f"Erro ao calcular similaridade sem√¢ntica: {e}")
            raise

    def format_metrics_report(
        self,
        dataset_type: str,
        metrics: Dict[str, Any]
    ) -> str:
        """
        Formata m√©tricas para exibi√ß√£o leg√≠vel.

        Args:
            dataset_type: 'closed' ou 'open'
            metrics: Dict com m√©tricas

        Returns:
            String formatada para exibi√ß√£o
        """
        report = ""

        if dataset_type == 'closed' or dataset_type == DatasetType.CLOSED.value:
            report += "üìä M√âTRICAS DE CLASSIFICA√á√ÉO\n"
            report += "=" * 60 + "\n"
            report += f"Acur√°cia:  {metrics.get('accuracy', 0):.2%}\n"
            report += f"Precis√£o:  {metrics.get('precision', 0):.2%}\n"
            report += f"Recall:    {metrics.get('recall', 0):.2%}\n"
            report += f"F1-Score:  {metrics.get('f1', 0):.2%}\n"

        else:  # open
            report += "üìù M√âTRICAS DE GERA√á√ÉO DE TEXTO\n"
            report += "=" * 60 + "\n"

            if metrics.get('bleu') is not None:
                report += f"BLEU:                  {metrics['bleu']:.4f}\n"
            if metrics.get('rouge_l') is not None:
                report += f"ROUGE-L:               {metrics['rouge_l']:.4f}\n"
            if metrics.get('semantic_similarity') is not None:
                report += f"Similaridade Sem√¢ntica: {metrics['semantic_similarity']:.4f}\n"

            report += f"\nItems avaliados: {metrics.get('count', 0)}\n"

        return report
