import os  # Para acessar vari√°veis de ambiente (API Keys)
import logging  # Para registrar o que est√° acontecendo (logs)
from typing import List, Tuple, Dict, Any, Optional  # Tipos para garantir que os dados estejam corretos

# --- Importa√ß√µes do N√∫cleo do openCHA ---
from openCHA.datapipes import DatapipeType  # Tipos de mem√≥ria
from openCHA.interface import Interface  # Interface gr√°fica (Gradio/Streamlit)
from openCHA.llms import LLMType  # Tipos de LLMs suportados (GPT, Gemini, etc)
from openCHA.orchestrator import Orchestrator  # O "C√©rebro" que pensa e age
from openCHA.planners import Action  # A√ß√µes que o agente pode tomar
from openCHA.planners import PlannerType  # Estrat√©gias de planejamento (ex: Tree of Thought)
from openCHA.response_generators import ResponseGeneratorType  # Como formatar a resposta
from openCHA.tasks import TASK_TO_CLASS  # Mapa de ferramentas dispon√≠veis (Google Search, Calc, etc)
from openCHA.utils import parse_addresses  # Utilit√°rio para achar arquivos na resposta
from pydantic import BaseModel, Field  # Valida√ß√£o de dados robusta

# --- A NOVA IMPORTA√á√ÉO CRUCIAL ---
# Importa a classe que criamos anteriormente para gerenciar m√∫ltiplos modelos em paralelo
from openCHA.llms.multi_llm_manager import MultiLLMManager

logger = logging.getLogger(__name__)  # Configura o logger deste arquivo


class openCHA(BaseModel):
    """
    Classe principal (Wrapper). Ela decide se vai rodar um agente simples
    ou uma compara√ß√£o complexa entre v√°rios agentes.
    """

    # --- Configura√ß√µes B√°sicas do Agente √önico ---
    name: str = "openCHA"  # Nome do agente
    # Lista de a√ß√µes passadas (mem√≥ria de curto prazo). Field(default_factory=list) √© a forma segura de criar listas vazias no Pydantic
    previous_actions: List[Action] = Field(default_factory=list)
    orchestrator: Optional[Orchestrator] = None  # O c√©rebro (inicialmente desligado/None)
    planner_llm: str = LLMType.OPENAI  # Qual IA vai planejar (padr√£o GPT)
    planner: str = PlannerType.TREE_OF_THOUGHT  # Qual estrat√©gia usar
    datapipe: str = DatapipeType.MEMORY  # Onde guardar mem√≥ria
    promptist: str = ""  # Otimizador de prompts (opcional)
    response_generator_llm: str = LLMType.OPENAI  # Qual IA vai escrever a resposta final
    response_generator: str = ResponseGeneratorType.BASE_GENERATOR  # Tipo de gerador
    meta: List[str] = Field(default_factory=list)  # Metadados (nomes de arquivos enviados)
    verbose: bool = False  # Se True, imprime tudo no terminal (debug)

    # --- NOVAS Configura√ß√µes para o MultiLLMManager ---
    multi_llm: Optional[MultiLLMManager] = None  # O gerenciador de m√∫ltiplos modelos (inicialmente None)

    # Configura√ß√µes que ser√£o passadas para o MultiLLMManager quando ele for criado
    multi_llm_enable_cache: bool = True  # Salvar respostas para economizar $
    multi_llm_timeout: int = 180  # Tempo limite (60s porque Tree of Thought demora mais)
    multi_llm_max_workers: int = 3  # Quantos modelos rodam ao mesmo tempo
    multi_llm_enable_retry: bool = True  # Tentar de novo se falhar
    multi_llm_retry_attempts: int = 2  # Quantas tentativas extras

    class Config:
        """Permite que o Pydantic aceite tipos complexos (como a classe Orchestrator)."""
        arbitrary_types_allowed = True

    def _generate_history(
        self,
        chat_history: Optional[List[Tuple[str, str]]] = None
    ) -> str:
        """
        Formata a lista de mensagens [('oi', 'ol√°')] em um texto √∫nico para a IA ler.
        """
        if chat_history is None:
            chat_history = []

        # Cria uma string longa separando User e CHA (Agente)
        history = "".join(
            [
                f"\n------------\nUser: {chat[0]}\nCHA: {chat[1]}\n------------\n"
                for chat in chat_history
            ]
        )
        return history

    def get_multi_llm(self) -> MultiLLMManager:
        """
        PADR√ÉO SINGLETON (Inicia√ß√£o Pregui√ßosa):
        S√≥ cria o MultiLLMManager se ele ainda n√£o existir.
        Isso economiza mem√≥ria se o usu√°rio s√≥ quiser usar o modo simples.
        """
        if self.multi_llm is None:
            logger.info("Inicializando MultiLLMManager COM ORQUESTRA√á√ÉO COMPLETA...")
            # Instancia a classe importada passando as configs definidas acima
            self.multi_llm = MultiLLMManager(
                enable_cache=self.multi_llm_enable_cache,
                default_timeout=self.multi_llm_timeout,
                max_workers=self.multi_llm_max_workers,
                enable_retry=self.multi_llm_enable_retry,
                retry_attempts=self.multi_llm_retry_attempts,
            )
            logger.info("MultiLLMManager inicializado com sucesso")
        return self.multi_llm

    def compare_llm_responses_full(
        self,
        query: str,
        models: Optional[List[str]] = None,
        parallel: bool = True,
        **kwargs
    ) -> Dict[str, Any]:
        """
        M√©todo Wrapper: Pega o pedido do usu√°rio e repassa para o MultiLLMManager.
        √â aqui que a m√°gica da compara√ß√£o acontece.
        """
        if not query or not query.strip():
            raise ValueError("Query n√£o pode estar vazia")

        logger.info(f"Comparando respostas (COM ORQUESTRA√á√ÉO) para query: {query[:100]}...")

        # Pega (ou cria) o gerenciador
        manager = self.get_multi_llm()

        # Chama o m√©todo que criamos no outro arquivo
        result = manager.generate_all_with_orchestration(
            query=query,
            models=models,
            parallel=parallel, # Define se roda tudo junto ou um por um
            **kwargs # Passa args extras (temperature, etc)
        )

        logger.info(
            f"Compara√ß√£o conclu√≠da: {result['metadata']['success_count']} sucessos"
        )

        return result

    def compare_and_analyze_full(self, query: str, models: Optional[List[str]] = None, **kwargs) -> Dict[str, Any]:
        """
        Vers√£o mais detalhada da compara√ß√£o, retornando estat√≠sticas de tempo.
        """
        manager = self.get_multi_llm()
        return manager.compare_responses_with_orchestration(query, models=models, **kwargs)

    def _run(
        self,
        query: str,
        chat_history: Optional[List[Tuple[str, str]]] = None,
        tasks_list: Optional[List[str]] = None,
        use_history: bool = False,
        **kwargs,
    ) -> str:
        """
        MODO CL√ÅSSICO (Single Agent):
        Executa a l√≥gica original do openCHA para um √∫nico agente.
        """
        if chat_history is None: chat_history = []
        if tasks_list is None: tasks_list = []

        # Prepara o texto do hist√≥rico
        history = self._generate_history(chat_history=chat_history)

        # Se o 'c√©rebro' (orchestrator) n√£o existe, cria um agora.
        if self.orchestrator is None:
            logger.info("Inicializando Orchestrator (MODO NORMAL)...")
            self.orchestrator = Orchestrator.initialize(
                planner_llm=self.planner_llm,
                planner_name=self.planner, # Ex: Tree of Thought
                datapipe_name=self.datapipe,
                promptist_name=self.promptist,
                response_generator_llm=self.response_generator_llm,
                response_generator_name=self.response_generator,
                available_tasks=tasks_list, # Ferramentas que ele pode usar
                previous_actions=self.previous_actions,
                verbose=self.verbose,
                **kwargs,
            )
            logger.info("Orchestrator inicializado")

        # Manda o agente executar a tarefa
        response = self.orchestrator.run(
            query=query,
            meta=self.meta,
            history=history,
            use_history=use_history,
            **kwargs,
        )

        return response

    def respond(
        self,
        message: str,
        openai_api_key_input: str,
        serp_api_key_input: str,
        gemini_api_key_input: str,
        deepseek_api_key_input: str,
        chat_history: List[Tuple[str, str]],
        check_box: bool,
        tasks_list: List[str],
    ) -> Tuple[str, List[Tuple[str, str]]]:
        """
        Callback para Interface Gr√°fica (UI).
        Recebe as chaves de API da tela e configura o ambiente.
        """
        # Configura vari√°veis de ambiente globais com as chaves digitadas
        os.environ["OPENAI_API_KEY"] = openai_api_key_input
        os.environ["SERP_API_KEY"] = serp_api_key_input
        os.environ["GEMINI_API_KEY"] = gemini_api_key_input
        os.environ["DEEPSEEK_API_KEY"] = deepseek_api_key_input

        try:
            # Chama o m√©todo _run (Single Agent) para processar a mensagem
            response = self._run(
                query=message,
                chat_history=chat_history,
                tasks_list=tasks_list,
                use_history=check_box,
            )

            # Verifica se a resposta cont√©m caminhos de arquivos gerados
            files = parse_addresses(response)

            if len(files) == 0:
                # Se for s√≥ texto, adiciona ao chat
                chat_history.append((message, response))
            else:
                # Se tiver arquivos, formata para a UI mostrar o download
                for i in range(len(files)):
                    chat_history.append(
                        (
                            message if i == 0 else None,
                            response[: files[i][1]], # Texto antes do arquivo
                        )
                    )
                    chat_history.append((None, (files[i][0],))) # O arquivo em si
                    response = response[files[i][2] :] # Texto depois do arquivo

            return "", chat_history

        except Exception as e:
            # Tratamento de erro para n√£o travar a tela do usu√°rio
            error_msg = f"Erro ao processar mensagem: {str(e)}"
            logger.error(error_msg, exc_info=True)
            chat_history.append((message, f"‚ùå {error_msg}"))
            return "", chat_history

    def reset(self) -> None:
        """Limpa tudo para come√ßar do zero."""
        logger.info("Resetando estado do openCHA...")
        self.previous_actions = []
        self.meta = []
        self.orchestrator = None  # Destr√≥i o orchestrator atual

        # Se o gerenciador multi-LLM existir, limpa o cache dele tamb√©m
        if self.multi_llm is not None:
            self.multi_llm.clear_cache()

        logger.info("Estado resetado com sucesso")

    def run_with_interface(self) -> None:
        """Lan√ßa a interface visual."""
        logger.info("Iniciando interface gr√°fica...")
        # Pega a lista de nomes de tarefas dispon√≠veis
        available_tasks = [key.value for key in TASK_TO_CLASS.keys()]
        interface = Interface()
        # Configura a UI passando os m√©todos desta classe como callbacks
        interface.prepare_interface(
            respond=self.respond,
            reset=self.reset,
            upload_meta=self.upload_meta,
            available_tasks=available_tasks,
        )

    def upload_meta(self, history: List[Tuple], file: Any) -> List[Tuple]:
        """Lida com upload de arquivos na UI."""
        # Adiciona o arquivo visualmente ao chat
        history = history + [((file.name,), None)]
        # Salva o nome do arquivo na lista de meta-dados do agente
        self.meta.append(file.name)
        logger.info(f"Arquivo uploaded: {file.name}")
        return history

    def run(
        self,
        query: str,
        chat_history: Optional[List[Tuple[str, str]]] = None,
        available_tasks: Optional[List[str]] = None,
        use_history: bool = False,
        use_multi_llm: bool = False, # FLAG NOVA
        compare_models: Optional[List[str]] = None, # Argumento NOVO
        **kwargs,
    ) -> str:
        """
        O NOVO PONTO DE ENTRADA PRINCIPAL.
        Decide se roda o modo normal ou o modo de compara√ß√£o (Multi-LLM).
        """
        if chat_history is None: chat_history = []
        if available_tasks is None: available_tasks = []

        try:
            # --- DECIS√ÉO DE ROTEAMENTO ---
            # Se o usu√°rio pediu 'use_multi_llm=True', vai para o modo compara√ß√£o
            if use_multi_llm:
                logger.info("Executando em MODO COMPARA√á√ÉO COM ORQUESTRA√á√ÉO COMPLETA")

                # Chama a compara√ß√£o completa
                results = self.compare_llm_responses_full(
                    query,
                    models=compare_models,
                    **kwargs
                )
                # Formata o dicion√°rio complexo em uma string bonita para o usu√°rio ler
                return self._format_multi_llm_results(results)

            # --- MODO PADR√ÉO ---
            # Se n√£o, roda apenas o _run normal (um agente)
            return self._run(
                query=query,
                chat_history=chat_history,
                tasks_list=available_tasks,
                use_history=use_history,
                **kwargs,
            )

        except Exception as e:
            error_msg = f"Erro ao executar query: {str(e)}"
            logger.error(error_msg, exc_info=True)
            return f"‚ùå {error_msg}"

    def _format_multi_llm_results(self, results: Dict[str, Any]) -> str:
        """
        Transforma o JSON de resultados em um relat√≥rio de texto leg√≠vel.
        Exibe tempos de planejamento e execu√ß√£o separadamente.
        """
        output_lines = [
            "=" * 80,
            "COMPARA√á√ÉO ENTRE M√öLTIPLOS LLMs (COM ORQUESTRA√á√ÉO COMPLETA)",
            "=" * 80,
            ""
        ]

        # Cabe√ßalho com totais
        metadata = results['metadata']
        output_lines.extend([
            f"‚è±Ô∏è  Tempo total: {metadata['total_time_ms']} ms",
            f"‚úÖ Sucessos: {metadata['success_count']} | ‚ùå Falhas: {metadata['failed_count']}",
            f"üî§ Tokens estimados: {metadata['total_tokens_estimate']}",
            f"üß† Tipo de execu√ß√£o: {metadata['execution_type']}",
            ""
        ])

        # Loop para formatar cada modelo individualmente
        for model_name, response in results['responses'].items():
            # Extrai m√©tricas
            time_ms = results['times'][model_name]
            planning_time = results['planning_times'][model_name] # Tempo pensando
            generation_time = results['generation_times'][model_name] # Tempo escrevendo
            error = results['errors'][model_name]

            output_lines.extend([
                f"{'=' * 80}",
                f"ü§ñ {model_name.upper()}", # Nome do modelo em destaque
                f"{'=' * 80}",
            ])

            if error:
                output_lines.append(f"‚ùå Erro: {error}")
            else:
                output_lines.extend([
                    f"‚è±Ô∏è  Tempo total: {time_ms} ms",
                    f"  ‚îú‚îÄ üß† Planejamento: {planning_time:.1f} ms", # Exibe tempo de pensamento
                    f"  ‚îî‚îÄ ‚úçÔ∏è  Gera√ß√£o: {generation_time:.1f} ms",    # Exibe tempo de escrita
                    f"üìù Resposta:",
                    f"{response}", # O texto gerado
                ])

            output_lines.append("")

        # Rodap√© com o vencedor de velocidade
        valid_times = {k: v for k, v in results['times'].items() if v is not None}
        if valid_times:
            fastest = min(valid_times.items(), key=lambda x: x[1])
            output_lines.extend([
                f"{'=' * 80}",
                f"üèÜ Modelo mais r√°pido: {fastest[0].upper()} ({fastest[1]} ms)",
                f"{'=' * 80}",
            ])

        return "\n".join(output_lines)

    def get_available_models(self) -> List[str]:
        """Helper para saber quais modelos posso chamar."""
        manager = self.get_multi_llm()
        return manager.get_available_models()

    def clear_multi_llm_cache(self) -> None:
        """Limpa cache especificamente do MultiLLM."""
        if self.multi_llm is not None:
            self.multi_llm.clear_cache()
            logger.info("Cache do MultiLLMManager limpo")
        else:
            logger.warning("MultiLLMManager n√£o foi inicializado ainda")
