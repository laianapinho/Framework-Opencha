import os  # Para acessar variÃ¡veis de ambiente (API Keys)
import logging  # Para registrar o que estÃ¡ acontecendo (logs)
from typing import List, Tuple, Dict, Any, Optional  # Tipos para garantir que os dados estejam corretos

# --- ImportaÃ§Ãµes do NÃºcleo do openCHA ---
from openCHA.datapipes import DatapipeType  # Tipos de memÃ³ria
from openCHA.interface import Interface  # Interface grÃ¡fica (Gradio/Streamlit)
from openCHA.llms import LLMType  # Tipos de LLMs suportados (GPT, Gemini, etc)
from openCHA.orchestrator import Orchestrator  # O "CÃ©rebro" que pensa e age
from openCHA.planners import Action  # AÃ§Ãµes que o agente pode tomar
from openCHA.planners import PlannerType  # EstratÃ©gias de planejamento (ex: Tree of Thought)
from openCHA.response_generators import ResponseGeneratorType  # Como formatar a resposta
from openCHA.tasks import TASK_TO_CLASS  # Mapa de ferramentas disponÃ­veis (Google Search, Calc, etc)
from openCHA.utils import parse_addresses  # UtilitÃ¡rio para achar arquivos na resposta
from pydantic import BaseModel, Field  # ValidaÃ§Ã£o de dados robusta

# --- A NOVA IMPORTAÃ‡ÃƒO CRUCIAL ---
# Importa a classe que criamos anteriormente para gerenciar mÃºltiplos modelos em paralelo
from openCHA.llms.multi_llm_manager import MultiLLMManager

logger = logging.getLogger(__name__)  # Configura o logger deste arquivo


class openCHA(BaseModel):
    """
    Classe principal do openCHA - Sistema de IA com OrquestraÃ§Ã£o Completa e Multi-LLM.

    âœ… OTIMIZADO: Usa TreeOfThoughtPlanner com timeout aumentado

    Decide se vai rodar um agente simples (modo normal) ou uma comparaÃ§Ã£o complexa
    entre vÃ¡rios agentes (modo Multi-LLM).

    Recursos:
        - Modo Normal: Um agente com orquestraÃ§Ã£o completa (Tree of Thought)
        - Modo Multi-LLM: ComparaÃ§Ã£o entre mÃºltiplos modelos (ChatGPT, Gemini, DeepSeek)
        - Interface grÃ¡fica integrada (Gradio)
        - Suporte a upload de arquivos
        - HistÃ³rico de conversaÃ§Ã£o
        - Cache inteligente
        - Retry automÃ¡tico em falhas
        - Timeout aumentado para TreeOfThought

    Exemplos:
        >>> # Modo Normal
        >>> agent = openCHA()
        >>> response = agent.run(query="Explique IA", use_multi_llm=False)
        >>>
        >>> # Modo Multi-LLM
        >>> response = agent.run(
        ...     query="Explique IA",
        ...     use_multi_llm=True,
        ...     compare_models=["chatgpt", "gemini"]
        ... )
        >>>
        >>> # Interface GrÃ¡fica
        >>> agent.run_with_interface()
    """

    # --- ConfiguraÃ§Ãµes BÃ¡sicas do Agente Ãšnico ---
    name: str = "openCHA"  # Nome do agente
    # Lista de aÃ§Ãµes passadas (memÃ³ria de curto prazo). Field(default_factory=list) Ã© a forma segura de criar listas vazias no Pydantic
    previous_actions: List[Action] = Field(default_factory=list)
    orchestrator: Optional[Orchestrator] = None  # O cÃ©rebro (inicialmente desligado/None)
    planner_llm: str = LLMType.OPENAI  # Qual IA vai planejar (padrÃ£o GPT)
    planner: str = PlannerType.TREE_OF_THOUGHT  # âœ… MANTÃ‰M Tree of Thought (Ã© o Ãºnico disponÃ­vel)
    datapipe: str = DatapipeType.MEMORY  # Onde guardar memÃ³ria
    promptist: str = ""  # Otimizador de prompts (opcional)
    response_generator_llm: str = LLMType.OPENAI  # Qual IA vai escrever a resposta final
    response_generator: str = ResponseGeneratorType.BASE_GENERATOR  # Tipo de gerador
    meta: List[str] = Field(default_factory=list)  # Metadados (nomes de arquivos enviados)
    verbose: bool = False  # Se True, imprime tudo no terminal (debug)

    # --- NOVAS ConfiguraÃ§Ãµes para o MultiLLMManager ---
    multi_llm: Optional[MultiLLMManager] = None  # O gerenciador de mÃºltiplos modelos (inicialmente None)

    # âœ… OTIMIZADO: ConfiguraÃ§Ãµes aumentadas para Tree of Thought
    multi_llm_enable_cache: bool = True  # Salvar respostas para economizar $
    multi_llm_timeout: int = 500  # âœ… AUMENTADO: 180 segundos (3 minutos) para Tree of Thought
    multi_llm_max_workers: int = 3  # Quantos modelos rodam ao mesmo tempo
    multi_llm_enable_retry: bool = True  # âœ… ATIVADO: Tentar de novo se falhar
    multi_llm_retry_attempts: int = 2  # Quantas tentativas extras

    class Config:
        """Permite que o Pydantic aceite tipos complexos (como a classe Orchestrator)."""
        arbitrary_types_allowed = True

    def _generate_history(
        self,
        chat_history: Optional[List[Tuple[str, str]]] = None
    ) -> str:
        """
        Formata a lista de mensagens [('oi', 'olÃ¡')] em um texto Ãºnico para a IA ler.

        Args:
            chat_history: Lista de tuplas (mensagem_usuario, resposta_agente)

        Returns:
            str: HistÃ³rico formatado como texto
        """
        if chat_history is None:
            chat_history = []

        # Cria uma string longa separando User e CHA (Agente)
        history = "".join(
            [
                f"\n------------\nUser: {chat[0]}\nCHA: {chat[1]}\n------------\n"
                for chat in chat_history
            ]
        )
        return history

    def get_multi_llm(self) -> MultiLLMManager:
        """
        PADRÃƒO SINGLETON (IniciaÃ§Ã£o PreguiÃ§osa):
        SÃ³ cria o MultiLLMManager se ele ainda nÃ£o existir.
        Isso economiza memÃ³ria se o usuÃ¡rio sÃ³ quiser usar o modo simples.

        Returns:
            MultiLLMManager: InstÃ¢ncia do gerenciador de mÃºltiplos modelos
        """
        if self.multi_llm is None:
            logger.info("Inicializando MultiLLMManager COM ORQUESTRAÃ‡ÃƒO COMPLETA...")
            # Instancia a classe importada passando as configs definidas acima
            self.multi_llm = MultiLLMManager(
                enable_cache=self.multi_llm_enable_cache,
                default_timeout=self.multi_llm_timeout,  # âœ… 180 segundos
                max_workers=self.multi_llm_max_workers,
                enable_retry=self.multi_llm_enable_retry,  # âœ… Retry ativado
                retry_attempts=self.multi_llm_retry_attempts,
            )
            logger.info("MultiLLMManager inicializado com sucesso")
        return self.multi_llm

    def compare_llm_responses_full(
        self,
        query: str,
        models: Optional[List[str]] = None,
        parallel: bool = True,
        **kwargs
    ) -> Dict[str, Any]:
        """
        MÃ©todo Wrapper: Pega o pedido do usuÃ¡rio e repassa para o MultiLLMManager.
        Ã‰ aqui que a mÃ¡gica da comparaÃ§Ã£o acontece.

        Args:
            query: Pergunta ou comando a ser processado
            models: Lista de modelos especÃ­ficos (None = todos disponÃ­veis)
            parallel: Se True, executa em paralelo; se False, sequencial
            **kwargs: ParÃ¢metros extras (temperature, max_tokens, etc)

        Returns:
            Dict[str, Any]: DicionÃ¡rio com estrutura MultiLLMResultFull contendo:
                - responses: Dict com respostas de cada modelo
                - times: Dict com tempos de execuÃ§Ã£o
                - planning_times: Dict com tempos de planejamento
                - generation_times: Dict com tempos de geraÃ§Ã£o
                - errors: Dict com erros (se houver)
                - metadata: EstatÃ­sticas agregadas

        Raises:
            ValueError: Se a query estiver vazia
        """
        if not query or not query.strip():
            raise ValueError("Query nÃ£o pode estar vazia")

        logger.info(f"Comparando respostas (COM ORQUESTRAÃ‡ÃƒO TREE OF THOUGHT) para query: {query[:100]}...")

        # Pega (ou cria) o gerenciador
        manager = self.get_multi_llm()

        # Chama o mÃ©todo que criamos no outro arquivo
        result = manager.generate_all_with_orchestration(
            query=query,
            models=models,
            parallel=parallel,  # Define se roda tudo junto ou um por um
            **kwargs  # Passa args extras (temperature, etc)
        )

        logger.info(
            f"ComparaÃ§Ã£o concluÃ­da: {result['metadata']['success_count']} sucessos, "
            f"{result['metadata']['failed_count']} falhas"
        )

        return result

    def compare_and_analyze_full(
        self,
        query: str,
        models: Optional[List[str]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        VersÃ£o mais detalhada da comparaÃ§Ã£o, retornando estatÃ­sticas de tempo
        e anÃ¡lise comparativa entre modelos.

        Args:
            query: Pergunta ou comando
            models: Lista de modelos especÃ­ficos (None = todos)
            **kwargs: ParÃ¢metros extras

        Returns:
            Dict[str, Any]: DicionÃ¡rio com anÃ¡lise comparativa incluindo:
                - query: Query original
                - responses: Respostas dos modelos
                - performance: MÃ©tricas detalhadas por modelo
                - summary: Resumo comparativo
        """
        manager = self.get_multi_llm()
        return manager.compare_responses_with_orchestration(query, models=models, **kwargs)

    def compare_llm_responses(
        self,
        query: str,
        models: Optional[List[str]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        â† NOVO MÃ‰TODO â†

        Wrapper simplificado para uso na Interface GrÃ¡fica (UI).
        Sempre executa em modo paralelo para melhor performance.

        Este mÃ©todo Ã© chamado pelo respond() quando o usuÃ¡rio ativa o Multi-LLM na UI.
        Ã‰ uma versÃ£o "amigÃ¡vel" do compare_llm_responses_full() que sempre usa
        as melhores configuraÃ§Ãµes para interface grÃ¡fica.

        Args:
            query: Pergunta a ser processada
            models: Lista de modelos a comparar (None = todos disponÃ­veis)
            **kwargs: ParÃ¢metros extras (temperature, max_tokens, etc)

        Returns:
            Dict[str, Any]: DicionÃ¡rio com estrutura MultiLLMResultFull

        Raises:
            ValueError: Se query estiver vazia ou modelos invÃ¡lidos

        Exemplos:
            >>> agent = openCHA()
            >>> results = agent.compare_llm_responses(
            ...     query="Explique Machine Learning",
            ...     models=["chatgpt", "gemini"]
            ... )
            >>> print(results['responses']['chatgpt'])
            >>> print(results['times']['chatgpt'])
        """
        logger.debug(f"compare_llm_responses() chamado para query: {query[:50]}...")

        return self.compare_llm_responses_full(
            query=query,
            models=models,
            parallel=True,  # UI sempre usa paralelo para velocidade
            **kwargs
        )

    def _run(
        self,
        query: str,
        chat_history: Optional[List[Tuple[str, str]]] = None,
        tasks_list: Optional[List[str]] = None,
        use_history: bool = False,
        **kwargs,
    ) -> str:
        """
        MODO CLÃSSICO (Single Agent):
        Executa a lÃ³gica original do openCHA para um Ãºnico agente.

        âœ… OTIMIZADO: Usa TreeOfThoughtPlanner (Ãºnico disponÃ­vel)

        Args:
            query: Pergunta ou comando
            chat_history: HistÃ³rico da conversa
            tasks_list: Lista de ferramentas/tarefas disponÃ­veis
            use_history: Se True, usa contexto da conversa anterior
            **kwargs: ParÃ¢metros extras

        Returns:
            str: Resposta do agente
        """
        if chat_history is None:
            chat_history = []
        if tasks_list is None:
            tasks_list = []

        # Prepara o texto do histÃ³rico
        history = self._generate_history(chat_history=chat_history)

        # Se o 'cÃ©rebro' (orchestrator) nÃ£o existe, cria um agora
        if self.orchestrator is None:
            logger.info("Inicializando Orchestrator com TreeOfThoughtPlanner...")
            logger.info("â±ï¸  AVISO: Tree of Thought pode levar 5-30 segundos para responder")
            logger.info("   Isso Ã© NORMAL! O sistema estÃ¡ pensando em mÃºltiplas estratÃ©gias.")

            self.orchestrator = Orchestrator.initialize(
                planner_llm=self.planner_llm,
                planner_name=PlannerType.TREE_OF_THOUGHT,  # âœ… Usa Tree of Thought
                datapipe_name=self.datapipe,
                promptist_name=self.promptist,
                response_generator_llm=self.response_generator_llm,
                response_generator_name=self.response_generator,
                available_tasks=tasks_list,  # Ferramentas que ele pode usar
                previous_actions=self.previous_actions,
                verbose=self.verbose,
                **kwargs,
            )
            logger.info("Orchestrator inicializado com sucesso")

        # Manda o agente executar a tarefa
        response = self.orchestrator.run(
            query=query,
            meta=self.meta,
            history=history,
            use_history=use_history,
            **kwargs,
        )

        return response

    def respond(
        self,
        message: str,
        openai_api_key_input: str,
        serp_api_key_input: str,
        gemini_api_key_input: str,
        deepseek_api_key_input: str,
        chat_history: List[Tuple[str, str]],
        check_box: bool,
        tasks_list: List[str],
        use_multi_llm: bool = False,  # â† NOVO PARÃ‚METRO
        compare_models: Optional[List[str]] = None,  # â† NOVO PARÃ‚METRO
    ) -> Tuple[str, List[Tuple[str, str]]]:
        """
        â† MÃ‰TODO ATUALIZADO â†

        Callback para Interface GrÃ¡fica (UI).
        Recebe as chaves de API da tela e configura o ambiente.
        Agora suporta tanto modo normal quanto Multi-LLM!

        Args:
            message: Mensagem do usuÃ¡rio
            openai_api_key_input: API key da OpenAI
            serp_api_key_input: API key do SERP
            gemini_api_key_input: API key do Gemini
            deepseek_api_key_input: API key do DeepSeek
            chat_history: HistÃ³rico da conversa
            check_box: Flag para usar histÃ³rico
            tasks_list: Lista de tarefas disponÃ­veis
            use_multi_llm: Se True, ativa modo de comparaÃ§Ã£o entre LLMs
            compare_models: Lista de modelos a comparar (ex: ['chatgpt','gemini'])

        Returns:
            Tuple[str, List[Tuple[str, str]]]: Tupla (mensagem_limpa, chat_history_atualizado)
        """
        # Configura variÃ¡veis de ambiente globais com as chaves digitadas
        os.environ["OPENAI_API_KEY"] = openai_api_key_input
        os.environ["SERP_API_KEY"] = serp_api_key_input
        os.environ["GEMINI_API_KEY"] = gemini_api_key_input
        os.environ["DEEPSEEK_API_KEY"] = deepseek_api_key_input

        try:
            # --- ROTEAMENTO: MODO NORMAL OU MULTI-LLM ---
            if use_multi_llm:
                logger.info("ðŸŒ Respond: modo Multi-LLM ativado")
                logger.info(f"Modelos selecionados: {compare_models}")

                # Chama a comparaÃ§Ã£o de mÃºltiplos modelos
                results = self.compare_llm_responses(
                    query=message,
                    models=compare_models if compare_models else None,
                )

                # Formata os resultados em texto legÃ­vel
                response = self._format_multi_llm_results(results)

            else:
                # Modo normal: um Ãºnico agente
                logger.info("ðŸ¤– Respond: modo Normal (single agent) com TreeOfThought")
                response = self._run(
                    query=message,
                    chat_history=chat_history,
                    tasks_list=tasks_list,
                    use_history=check_box,
                )

            # Verifica se a resposta contÃ©m caminhos de arquivos gerados
            files = parse_addresses(response)

            if len(files) == 0:
                # Se for sÃ³ texto, adiciona ao chat
                chat_history.append((message, response))
            else:
                # Se tiver arquivos, formata para a UI mostrar o download
                for i in range(len(files)):
                    chat_history.append(
                        (
                            message if i == 0 else None,
                            response[: files[i][1]],  # Texto antes do arquivo
                        )
                    )
                    chat_history.append((None, (files[i][0],)))  # O arquivo em si
                    response = response[files[i][2] :]  # Texto depois do arquivo

            return "", chat_history

        except Exception as e:
            # Tratamento de erro para nÃ£o travar a tela do usuÃ¡rio
            error_msg = f"Erro ao processar mensagem: {str(e)}"
            logger.error(error_msg, exc_info=True)
            chat_history.append((message, f"âŒ {error_msg}"))
            return "", chat_history

    def reset(self) -> None:
        """
        Limpa tudo para comeÃ§ar do zero.
        Reseta o orchestrator, histÃ³rico de aÃ§Ãµes e cache do Multi-LLM.
        """
        logger.info("Resetando estado do openCHA...")
        self.previous_actions = []
        self.meta = []
        self.orchestrator = None  # DestrÃ³i o orchestrator atual

        # Se o gerenciador multi-LLM existir, limpa o cache dele tambÃ©m
        if self.multi_llm is not None:
            self.multi_llm.clear_cache()

        logger.info("Estado resetado com sucesso")

    def run_with_interface(self) -> None:
        """
        LanÃ§a a interface visual (Gradio).
        Configura todos os callbacks e inicia o servidor web.
        """
        logger.info("Iniciando interface grÃ¡fica...")
        # Pega a lista de nomes de tarefas disponÃ­veis
        available_tasks = [key.value for key in TASK_TO_CLASS.keys()]
        interface = Interface()
        # Configura a UI passando os mÃ©todos desta classe como callbacks
        interface.prepare_interface(
            respond=self.respond,
            reset=self.reset,
            upload_meta=self.upload_meta,
            available_tasks=available_tasks,
        )

    def upload_meta(self, history: List[Tuple], file: Any) -> List[Tuple]:
        """
        Lida com upload de arquivos na UI.

        Args:
            history: HistÃ³rico atual do chat
            file: Arquivo enviado pelo usuÃ¡rio

        Returns:
            List[Tuple]: HistÃ³rico atualizado com o arquivo
        """
        # Adiciona o arquivo visualmente ao chat
        history = history + [((file.name,), None)]
        # Salva o nome do arquivo na lista de meta-dados do agente
        self.meta.append(file.name)
        logger.info(f"Arquivo uploaded: {file.name}")
        return history

    def run(
        self,
        query: str,
        chat_history: Optional[List[Tuple[str, str]]] = None,
        available_tasks: Optional[List[str]] = None,
        use_history: bool = False,
        use_multi_llm: bool = False,  # FLAG NOVA
        compare_models: Optional[List[str]] = None,  # Argumento NOVO
        **kwargs,
    ) -> str:
        """
        O NOVO PONTO DE ENTRADA PRINCIPAL.
        Decide se roda o modo normal ou o modo de comparaÃ§Ã£o (Multi-LLM).

        âœ… OTIMIZADO: Usa TreeOfThoughtPlanner com timeout aumentado

        Args:
            query: Pergunta ou comando
            chat_history: HistÃ³rico da conversa
            available_tasks: Lista de tarefas/ferramentas disponÃ­veis
            use_history: Se True, usa contexto anterior
            use_multi_llm: Se True, ativa comparaÃ§Ã£o entre mÃºltiplos modelos
            compare_models: Lista de modelos a comparar
            **kwargs: ParÃ¢metros extras

        Returns:
            str: Resposta formatada (texto simples para modo normal,
                 relatÃ³rio comparativo para Multi-LLM)

        Examples:
            >>> # Modo Normal
            >>> agent = openCHA()
            >>> response = agent.run(
            ...     query="Explique IA",
            ...     use_multi_llm=False
            ... )
            >>>
            >>> # Modo Multi-LLM
            >>> response = agent.run(
            ...     query="Explique IA",
            ...     use_multi_llm=True,
            ...     compare_models=["chatgpt", "gemini"]
            ... )
        """
        if chat_history is None:
            chat_history = []
        if available_tasks is None:
            available_tasks = []

        try:
            # --- DECISÃƒO DE ROTEAMENTO ---
            # Se o usuÃ¡rio pediu 'use_multi_llm=True', vai para o modo comparaÃ§Ã£o
            if use_multi_llm:
                logger.info("ðŸŒ Executando em MODO COMPARAÃ‡ÃƒO COM ORQUESTRAÃ‡ÃƒO TREE OF THOUGHT")
                logger.info(f"Modelos: {compare_models if compare_models else 'todos disponÃ­veis'}")

                # Chama a comparaÃ§Ã£o completa
                results = self.compare_llm_responses_full(
                    query,
                    models=compare_models,
                    **kwargs
                )
                # Formata o dicionÃ¡rio complexo em uma string bonita para o usuÃ¡rio ler
                return self._format_multi_llm_results(results)

            # --- MODO PADRÃƒO ---
            # Se nÃ£o, roda apenas o _run normal (um agente)
            logger.info("ðŸ¤– Executando em MODO NORMAL (single agent com TreeOfThought)")
            return self._run(
                query=query,
                chat_history=chat_history,
                tasks_list=available_tasks,
                use_history=use_history,
                **kwargs,
            )

        except Exception as e:
            error_msg = f"Erro ao executar query: {str(e)}"
            logger.error(error_msg, exc_info=True)
            return f"âŒ {error_msg}"

    def _format_multi_llm_results(self, results: Dict[str, Any]) -> str:
        """
        Transforma o JSON de resultados em um relatÃ³rio de texto legÃ­vel.
        Exibe tempos de planejamento e execuÃ§Ã£o separadamente.

        Args:
            results: DicionÃ¡rio com estrutura MultiLLMResultFull

        Returns:
            str: RelatÃ³rio formatado em texto
        """
        output_lines = [
            "=" * 80,
            "COMPARAÃ‡ÃƒO ENTRE MÃšLTIPLOS LLMs (COM ORQUESTRAÃ‡ÃƒO TREE OF THOUGHT)",
            "=" * 80,
            ""
        ]

        # CabeÃ§alho com totais
        metadata = results['metadata']
        output_lines.extend([
            f"â±ï¸  Tempo total: {metadata['total_time_ms']} ms",
            f"âœ… Sucessos: {metadata['success_count']} | âŒ Falhas: {metadata['failed_count']}",
            f"ðŸ”¤ Tokens estimados: {metadata['total_tokens_estimate']}",
            f"ðŸ§  Tipo de execuÃ§Ã£o: {metadata['execution_type']}",
            ""
        ])

        # Loop para formatar cada modelo individualmente
        for model_name, response in results['responses'].items():
            # Extrai mÃ©tricas
            time_ms = results['times'][model_name]
            planning_time = results['planning_times'][model_name]  # Tempo pensando
            generation_time = results['generation_times'][model_name]  # Tempo escrevendo
            error = results['errors'][model_name]

            output_lines.extend([
                f"{'=' * 80}",
                f"ðŸ¤– {model_name.upper()}",  # Nome do modelo em destaque
                f"{'=' * 80}",
            ])

            if error:
                output_lines.append(f"âŒ Erro: {error}")
            else:
                output_lines.extend([
                    f"â±ï¸  Tempo total: {time_ms} ms",
                    f"  â”œâ”€ ðŸ§  Planejamento: {planning_time:.1f} ms",  # Exibe tempo de pensamento
                    f"  â””â”€ âœï¸  GeraÃ§Ã£o: {generation_time:.1f} ms",    # Exibe tempo de escrita
                    f"ðŸ“ Resposta:",
                    f"{response}",  # O texto gerado
                ])

            output_lines.append("")

        # RodapÃ© com o vencedor de velocidade
        valid_times = {k: v for k, v in results['times'].items() if v is not None}
        if valid_times:
            fastest = min(valid_times.items(), key=lambda x: x[1])
            output_lines.extend([
                f"{'=' * 80}",
                f"ðŸ† Modelo mais rÃ¡pido: {fastest[0].upper()} ({fastest[1]} ms)",
                f"{'=' * 80}",
            ])

        return "\n".join(output_lines)

    def get_available_models(self) -> List[str]:
        """
        Helper para saber quais modelos estÃ£o disponÃ­veis e funcionando.

        Returns:
            List[str]: Lista de nomes dos modelos disponÃ­veis
        """
        manager = self.get_multi_llm()
        return manager.get_available_models()

    def clear_multi_llm_cache(self) -> None:
        """
        Limpa cache especificamente do MultiLLM.
        Ãštil quando vocÃª quer respostas frescas mesmo para queries repetidas.
        """
        if self.multi_llm is not None:
            self.multi_llm.clear_cache()
            logger.info("Cache do MultiLLMManager limpo")
        else:
            logger.warning("MultiLLMManager nÃ£o foi inicializado ainda")
