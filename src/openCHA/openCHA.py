import os
import logging
from typing import List, Tuple, Dict, Any, Optional

from openCHA.datapipes import DatapipeType
from openCHA.interface import Interface
from openCHA.llms import LLMType
from openCHA.orchestrator import Orchestrator
from openCHA.planners import Action
from openCHA.planners import PlannerType
from openCHA.response_generators import ResponseGeneratorType
from openCHA.tasks import TASK_TO_CLASS
from openCHA.utils import parse_addresses
from pydantic import BaseModel, Field
from openCHA.llms.multi_llm_manager import MultiLLMManager

logger = logging.getLogger(__name__)


class openCHA(BaseModel):
    """
    Classe principal do framework openCHA para agentes conversacionais com IA.
    
    Recursos:
        - Orquestra√ß√£o de tarefas complexas
        - Planejamento com Tree of Thought
        - Gera√ß√£o de respostas contextualizadas
        - Suporte a m√∫ltiplos LLMs (ChatGPT, Gemini, DeepSeek)
        - Compara√ß√£o paralela entre modelos
        - Interface de usu√°rio integrada
        - Upload e processamento de arquivos
    
    Exemplos:
        >>> # Uso b√°sico
        >>> cha = openCHA()
        >>> resposta = cha.run("Explique intelig√™ncia artificial")
        >>>
        >>> # Comparar m√∫ltiplos modelos
        >>> comparacao = cha.compare_llm_responses(
        ...     "Qual a capital do Brasil?",
        ...     models=['chatgpt', 'gemini', 'deepseek']
        ... )
        >>>
        >>> # Com interface gr√°fica
        >>> cha.run_with_interface()
    """
    
    name: str = "openCHA"
    previous_actions: List[Action] = Field(default_factory=list)
    orchestrator: Optional[Orchestrator] = None
    planner_llm: str = LLMType.OPENAI
    planner: str = PlannerType.TREE_OF_THOUGHT
    datapipe: str = DatapipeType.MEMORY
    promptist: str = ""
    response_generator_llm: str = LLMType.OPENAI
    response_generator: str = ResponseGeneratorType.BASE_GENERATOR
    meta: List[str] = Field(default_factory=list)
    verbose: bool = False
    
    # Multi-LLM Manager para compara√ß√£o entre modelos
    multi_llm: Optional[MultiLLMManager] = None
    
    # Configura√ß√µes do Multi-LLM
    multi_llm_enable_cache: bool = True
    multi_llm_timeout: int = 30
    multi_llm_max_workers: int = 3
    multi_llm_enable_retry: bool = True
    multi_llm_retry_attempts: int = 2

    class Config:
        """Configura√ß√£o do Pydantic para permitir tipos arbitr√°rios."""
        arbitrary_types_allowed = True

    def _generate_history(
        self, 
        chat_history: Optional[List[Tuple[str, str]]] = None
    ) -> str:
        """
        Gera uma string formatada do hist√≥rico de conversa√ß√£o.
        
        Args:
            chat_history: Lista de tuplas (user_message, cha_response)
        
        Returns:
            str: Hist√≥rico formatado como string
        """
        if chat_history is None:
            chat_history = []

        history = "".join(
            [
                f"\n------------\nUser: {chat[0]}\nCHA: {chat[1]}\n------------\n"
                for chat in chat_history
            ]
        )
        return history
    
    def get_multi_llm(self) -> MultiLLMManager:
        """
        Retorna uma inst√¢ncia de MultiLLMManager.
        Se ainda n√£o existir, inicializa com as configura√ß√µes da classe.
        
        Returns:
            MultiLLMManager: Gerenciador de m√∫ltiplos LLMs configurado
        """
        if self.multi_llm is None:
            logger.info("Inicializando MultiLLMManager...")
            self.multi_llm = MultiLLMManager(
                enable_cache=self.multi_llm_enable_cache,
                default_timeout=self.multi_llm_timeout,
                max_workers=self.multi_llm_max_workers,
                enable_retry=self.multi_llm_enable_retry,
                retry_attempts=self.multi_llm_retry_attempts,
            )
            logger.info("MultiLLMManager inicializado com sucesso")
        return self.multi_llm

    def compare_llm_responses(
        self,
        query: str,
        models: Optional[List[str]] = None,
        parallel: bool = True,
        **kwargs
    ) -> Dict[str, Any]:
        """
        Compara respostas de m√∫ltiplos LLMs para a mesma query.
        
        Este m√©todo executa a query em ChatGPT, Gemini e DeepSeek simultaneamente
        e retorna as respostas, tempos de execu√ß√£o e poss√≠veis erros.
        
        Args:
            query: Pergunta ou prompt a ser executado
            models: Lista de modelos espec√≠ficos ['chatgpt', 'gemini', 'deepseek']
                   Se None, executa em todos os modelos dispon√≠veis
            parallel: Se True, executa em paralelo; se False, sequencial
            **kwargs: Par√¢metros adicionais como:
                - temperature (float): Criatividade do modelo (0-2)
                - max_tokens (int): Limite de tokens na resposta
                - top_p (float): Nucleus sampling
        
        Returns:
            Dict contendo:
                - responses: Dicion√°rio com respostas de cada modelo
                - times: Tempos de execu√ß√£o em milissegundos
                - errors: Erros encontrados (None se sucesso)
                - metadata: Informa√ß√µes adicionais (total_time, success_count, etc)
        
        Exemplos:
            >>> cha = openCHA()
            >>> resultado = cha.compare_llm_responses("Explique IA")
            >>> print(resultado['responses']['chatgpt'])
            >>> print(f"Tempo: {resultado['times']['chatgpt']} ms")
            >>>
            >>> # Com par√¢metros customizados
            >>> resultado = cha.compare_llm_responses(
            ...     "Escreva um poema",
            ...     models=['chatgpt', 'gemini'],
            ...     temperature=0.9,
            ...     max_tokens=500
            ... )
        """
        if not query or not query.strip():
            raise ValueError("Query n√£o pode estar vazia")
        
        logger.info(f"Comparando respostas de LLMs para query: {query[:100]}...")
        
        manager = self.get_multi_llm()
        result = manager.generate_all(
            query=query,
            models=models,
            parallel=parallel,
            **kwargs
        )
        
        logger.info(
            f"Compara√ß√£o conclu√≠da: {result['metadata']['success_count']} sucessos, "
            f"{result['metadata']['failed_count']} falhas"
        )
        
        return result
    
    def compare_and_analyze(
        self,
        query: str,
        models: Optional[List[str]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        Executa compara√ß√£o entre LLMs e retorna an√°lise detalhada.
        
        Al√©m das respostas, retorna m√©tricas como modelo mais r√°pido,
        resposta mais longa, e compara√ß√µes de performance.
        
        Args:
            query: Query a executar
            models: Modelos espec√≠ficos ou None para todos
            **kwargs: Par√¢metros adicionais
        
        Returns:
            Dict com an√°lise comparativa completa
        """
        manager = self.get_multi_llm()
        return manager.compare_responses(query, models=models, **kwargs)

    def _run(
        self,
        query: str,
        chat_history: Optional[List[Tuple[str, str]]] = None,
        tasks_list: Optional[List[str]] = None,
        use_history: bool = False,
        **kwargs,
    ) -> str:
        """
        Executa a query usando o orchestrator principal.
        
        Args:
            query: Pergunta ou comando do usu√°rio
            chat_history: Hist√≥rico de conversa√ß√£o
            tasks_list: Lista de tarefas dispon√≠veis
            use_history: Se True, inclui hist√≥rico no contexto
            **kwargs: Par√¢metros adicionais
        
        Returns:
            str: Resposta gerada pelo sistema
        """
        if chat_history is None:
            chat_history = []
        if tasks_list is None:
            tasks_list = []

        history = self._generate_history(chat_history=chat_history)

        # Inicializa orchestrator se necess√°rio
        if self.orchestrator is None:
            logger.info("Inicializando Orchestrator...")
            self.orchestrator = Orchestrator.initialize(
                planner_llm=self.planner_llm,
                planner_name=self.planner,
                datapipe_name=self.datapipe,
                promptist_name=self.promptist,
                response_generator_llm=self.response_generator_llm,
                response_generator_name=self.response_generator,
                available_tasks=tasks_list,
                previous_actions=self.previous_actions,
                verbose=self.verbose,
                **kwargs,
            )
            logger.info("Orchestrator inicializado")

        response = self.orchestrator.run(
            query=query,
            meta=self.meta,
            history=history,
            use_history=use_history,
            **kwargs,
        )

        return response

    def respond(
        self,
        message: str,
        openai_api_key_input: str,
        serp_api_key_input: str,
        gemini_api_key_input: str,
        deepseek_api_key_input: str,
        chat_history: List[Tuple[str, str]],
        check_box: bool,
        tasks_list: List[str],
    ) -> Tuple[str, List[Tuple[str, str]]]:
        """
        M√©todo de resposta usado pela interface gr√°fica.
        
        NOTA: Este m√©todo modifica os.environ globalmente, o que n√£o √© ideal
        para ambientes multi-usu√°rio. Considere usar gerenciamento de credenciais
        mais seguro em produ√ß√£o.
        
        Args:
            message: Mensagem do usu√°rio
            openai_api_key_input: API key da OpenAI
            serp_api_key_input: API key do SERP
            gemini_api_key_input: API key do Gemini
            deepseek_api_key_input: API key do DeepSeek
            chat_history: Hist√≥rico da conversa
            check_box: Flag para usar hist√≥rico
            tasks_list: Lista de tarefas dispon√≠veis
        
        Returns:
            Tupla (mensagem_vazia, chat_history_atualizado)
        """
        # Configura API keys (ATEN√á√ÉO: modifica ambiente global)
        os.environ["OPENAI_API_KEY"] = openai_api_key_input
        os.environ["SERP_API_KEY"] = serp_api_key_input  # Corrigido de SEPR
        os.environ["GEMINI_API_KEY"] = gemini_api_key_input
        os.environ["DEEPSEEK_API_KEY"] = deepseek_api_key_input
        
        try:
            response = self._run(
                query=message,
                chat_history=chat_history,
                tasks_list=tasks_list,
                use_history=check_box,
            )

            files = parse_addresses(response)

            if len(files) == 0:
                chat_history.append((message, response))
            else:
                # Processa arquivos na resposta
                for i in range(len(files)):
                    chat_history.append(
                        (
                            message if i == 0 else None,
                            response[: files[i][1]],
                        )
                    )
                    chat_history.append((None, (files[i][0],)))
                    response = response[files[i][2] :]

            return "", chat_history
            
        except Exception as e:
            error_msg = f"Erro ao processar mensagem: {str(e)}"
            logger.error(error_msg, exc_info=True)
            chat_history.append((message, f"‚ùå {error_msg}"))
            return "", chat_history

    def reset(self) -> None:
        """
        Reseta o estado do sistema, limpando hist√≥rico e a√ß√µes anteriores.
        """
        logger.info("Resetando estado do openCHA...")
        self.previous_actions = []
        self.meta = []
        self.orchestrator = None  # For√ßa reinicializa√ß√£o
        
        # Limpa cache do multi-LLM se existir
        if self.multi_llm is not None:
            self.multi_llm.clear_cache()
        
        logger.info("Estado resetado com sucesso")

    def run_with_interface(self) -> None:
        """
        Inicia a interface gr√°fica do openCHA.
        
        A interface permite intera√ß√£o via browser com upload de arquivos,
        sele√ß√£o de tarefas e chat interativo.
        """
        logger.info("Iniciando interface gr√°fica...")
        available_tasks = [key.value for key in TASK_TO_CLASS.keys()]
        interface = Interface()
        interface.prepare_interface(
            respond=self.respond,
            reset=self.reset,
            upload_meta=self.upload_meta,
            available_tasks=available_tasks,
        )

    def upload_meta(
        self, 
        history: List[Tuple], 
        file: Any
    ) -> List[Tuple]:
        """
        Processa upload de arquivo e adiciona ao hist√≥rico.
        
        Args:
            history: Hist√≥rico atual
            file: Objeto de arquivo uploaded
        
        Returns:
            Hist√≥rico atualizado com o arquivo
        """
        history = history + [((file.name,), None)]
        self.meta.append(file.name)
        logger.info(f"Arquivo uploaded: {file.name}")
        return history

    def run(
        self,
        query: str,
        chat_history: Optional[List[Tuple[str, str]]] = None,
        available_tasks: Optional[List[str]] = None,
        use_history: bool = False,
        use_multi_llm: bool = False,
        compare_models: Optional[List[str]] = None,
        **kwargs,
    ) -> str:
        """
        M√©todo principal para executar queries no openCHA.
        
        Args:
            query: Pergunta ou comando do usu√°rio
            chat_history: Hist√≥rico de conversa√ß√£o anterior
            available_tasks: Lista de tarefas que o sistema pode usar
            use_history: Se True, inclui hist√≥rico no contexto
            use_multi_llm: Se True, usa compara√ß√£o entre m√∫ltiplos LLMs
            compare_models: Modelos espec√≠ficos para compara√ß√£o
            **kwargs: Par√¢metros adicionais
        
        Returns:
            str: Resposta gerada ou compara√ß√£o formatada
        
        Examples:
            >>> cha = openCHA()
            >>> 
            >>> # Uso normal com orchestrator
            >>> resposta = cha.run("Qual √© a capital do Brasil?")
            >>> 
            >>> # Com hist√≥rico
            >>> resposta = cha.run(
            ...     "E a popula√ß√£o?",
            ...     chat_history=[("Qual √© a capital do Brasil?", "Bras√≠lia")],
            ...     use_history=True
            ... )
            >>>
            >>> # Compara√ß√£o entre modelos
            >>> resposta = cha.run(
            ...     "Explique computa√ß√£o qu√¢ntica",
            ...     use_multi_llm=True,
            ...     compare_models=['chatgpt', 'gemini']
            ... )
        """
        if chat_history is None:
            chat_history = []
        if available_tasks is None:
            available_tasks = []

        try:
            # Modo de compara√ß√£o multi-LLM
            if use_multi_llm:
                logger.info("Executando em modo multi-LLM comparison")
                results = self.compare_llm_responses(
                    query,
                    models=compare_models,
                    **kwargs
                )
                return self._format_multi_llm_results(results)
            
            # Modo normal com orchestrator
            return self._run(
                query=query,
                chat_history=chat_history,
                tasks_list=available_tasks,
                use_history=use_history,
                **kwargs,
            )
            
        except Exception as e:
            error_msg = f"Erro ao executar query: {str(e)}"
            logger.error(error_msg, exc_info=True)
            return f"‚ùå {error_msg}"
    
    def _format_multi_llm_results(self, results: Dict[str, Any]) -> str:
        """
        Formata resultados da compara√ß√£o multi-LLM em string leg√≠vel.
        
        Args:
            results: Dicion√°rio retornado por compare_llm_responses
        
        Returns:
            str: Resultados formatados
        """
        output_lines = [
            "=" * 80,
            "COMPARA√á√ÉO ENTRE M√öLTIPLOS LLMs",
            "=" * 80,
            ""
        ]
        
        # Informa√ß√µes gerais
        metadata = results['metadata']
        output_lines.extend([
            f"‚è±Ô∏è  Tempo total: {metadata['total_time_ms']} ms",
            f"‚úÖ Sucessos: {metadata['success_count']} | ‚ùå Falhas: {metadata['failed_count']}",
            f"üî§ Tokens estimados: {metadata['total_tokens_estimate']}",
            ""
        ])
        
        # Respostas de cada modelo
        for model_name, response in results['responses'].items():
            time_ms = results['times'][model_name]
            error = results['errors'][model_name]
            
            output_lines.extend([
                f"{'=' * 80}",
                f"ü§ñ {model_name.upper()}",
                f"{'=' * 80}",
            ])
            
            if error:
                output_lines.append(f"‚ùå Erro: {error}")
            else:
                output_lines.extend([
                    f"‚è±Ô∏è  Tempo: {time_ms} ms",
                    f"üìù Resposta:",
                    f"{response}",
                ])
            
            output_lines.append("")
        
        # Identificar modelo mais r√°pido
        valid_times = {k: v for k, v in results['times'].items() if v is not None}
        if valid_times:
            fastest = min(valid_times.items(), key=lambda x: x[1])
            output_lines.extend([
                f"{'=' * 80}",
                f"üèÜ Modelo mais r√°pido: {fastest[0].upper()} ({fastest[1]} ms)",
                f"{'=' * 80}",
            ])
        
        return "\n".join(output_lines)
    
    def get_available_models(self) -> List[str]:
        """
        Retorna lista de modelos LLM dispon√≠veis para compara√ß√£o.
        
        Returns:
            List[str]: Nomes dos modelos dispon√≠veis
        """
        manager = self.get_multi_llm()
        return manager.get_available_models()
    
    def clear_multi_llm_cache(self) -> None:
        """
        Limpa o cache do MultiLLMManager.
        √ötil para for√ßar novas requisi√ß√µes aos modelos.
        """
        if self.multi_llm is not None:
            self.multi_llm.clear_cache()
            logger.info("Cache do MultiLLMManager limpo")
        else:
            logger.warning("MultiLLMManager n√£o foi inicializado ainda")