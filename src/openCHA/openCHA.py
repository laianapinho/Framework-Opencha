import os
import logging
from typing import List, Tuple, Dict, Any, Optional

from openCHA.datapipes import DatapipeType
from openCHA.interface import Interface
from openCHA.llms import LLMType
from openCHA.orchestrator import Orchestrator
from openCHA.planners import Action
from openCHA.planners import PlannerType
from openCHA.response_generators import ResponseGeneratorType
from openCHA.tasks import TASK_TO_CLASS
from openCHA.utils import parse_addresses
from pydantic import BaseModel, Field

from openCHA.llms.multi_llm_manager import MultiLLMManager

logger = logging.getLogger(__name__)


class openCHA(BaseModel):
    """
    Classe principal do openCHA - Sistema de IA com OrquestraÃ§Ã£o Completa e Multi-LLM.

    âœ… OTIMIZADO: Usa TreeOfThoughtPlanner com timeout aumentado

    Decide se vai rodar um agente simples (modo normal) ou uma comparaÃ§Ã£o complexa
    entre vÃ¡rios agentes (modo Multi-LLM).

    Recursos:
        - Modo Normal: Um agente com orquestraÃ§Ã£o completa (Tree of Thought)
        - Modo Multi-LLM: ComparaÃ§Ã£o entre mÃºltiplos modelos (ChatGPT, Gemini, DeepSeek)
        - Interface grÃ¡fica integrada (Gradio)
        - Suporte a upload de arquivos
        - HistÃ³rico de conversaÃ§Ã£o
        - Cache inteligente
        - Retry automÃ¡tico em falhas
        - Timeout aumentado para TreeOfThought

    Exemplos:
        >>> # Modo Normal
        >>> agent = openCHA()
        >>> response = agent.run(query="Explique IA", use_multi_llm=False)
        >>>
        >>> # Modo Multi-LLM
        >>> response = agent.run(
        ...     query="Explique IA",
        ...     use_multi_llm=True,
        ...     compare_models=["chatgpt", "gemini"]
        ... )
        >>>
        >>> # Interface GrÃ¡fica
        >>> agent.run_with_interface()
    """

    name: str = "openCHA"
    previous_actions: List[Action] = Field(default_factory=list)
    orchestrator: Optional[Orchestrator] = None
    planner_llm: str = LLMType.OPENAI
    planner: str = PlannerType.TREE_OF_THOUGHT
    datapipe: str = DatapipeType.MEMORY
    promptist: str = ""
    response_generator_llm: str = LLMType.OPENAI
    response_generator: str = ResponseGeneratorType.BASE_GENERATOR
    meta: List[str] = Field(default_factory=list)
    verbose: bool = False

    multi_llm: Optional[MultiLLMManager] = None

    multi_llm_enable_cache: bool = True
    multi_llm_timeout: int = 500
    multi_llm_max_workers: int = 3
    multi_llm_enable_retry: bool = True
    multi_llm_retry_attempts: int = 2

    class Config:
        arbitrary_types_allowed = True

    def _generate_history(
        self,
        chat_history: Optional[List[Tuple[str, str]]] = None
    ) -> str:
        """
        Formata a lista de mensagens [('oi', 'olÃ¡')] em um texto Ãºnico para a IA ler.

        Args:
            chat_history: Lista de tuplas (mensagem_usuario, resposta_agente)

        Returns:
            str: HistÃ³rico formatado como texto
        """
        if chat_history is None:
            chat_history = []

        history = "".join(
            [
                f"\n------------\nUser: {chat[0]}\nCHA: {chat[1]}\n------------\n"
                for chat in chat_history
            ]
        )
        return history

    def get_multi_llm(self) -> MultiLLMManager:
        """
        PADRÃƒO SINGLETON (IniciaÃ§Ã£o PreguiÃ§osa):
        SÃ³ cria o MultiLLMManager se ele ainda nÃ£o existir.
        Isso economiza memÃ³ria se o usuÃ¡rio sÃ³ quiser usar o modo simples.

        Returns:
            MultiLLMManager: InstÃ¢ncia do gerenciador de mÃºltiplos modelos
        """
        if self.multi_llm is None:
            logger.info("Inicializando MultiLLMManager COM ORQUESTRAÃ‡ÃƒO COMPLETA...")
            self.multi_llm = MultiLLMManager(
                enable_cache=self.multi_llm_enable_cache,
                default_timeout=self.multi_llm_timeout,
                max_workers=self.multi_llm_max_workers,
                enable_retry=self.multi_llm_enable_retry,
                retry_attempts=self.multi_llm_retry_attempts,
            )
            logger.info("MultiLLMManager inicializado com sucesso")
        return self.multi_llm

    def compare_llm_responses_full(
        self,
        query: str,
        models: Optional[List[str]] = None,
        parallel: bool = True,
        **kwargs
    ) -> Dict[str, Any]:
        """
        MÃ©todo Wrapper: Pega o pedido do usuÃ¡rio e repassa para o MultiLLMManager.
        Ã‰ aqui que a mÃ¡gica da comparaÃ§Ã£o acontece.

        Args:
            query: Pergunta ou comando a ser processado
            models: Lista de modelos especÃ­ficos (None = todos disponÃ­veis)
            parallel: Se True, executa em paralelo; se False, sequencial
            **kwargs: ParÃ¢metros extras (temperature, max_tokens, etc)

        Returns:
            Dict[str, Any]: DicionÃ¡rio com estrutura MultiLLMResultFull contendo:
                - responses: Dict com respostas de cada modelo
                - times: Dict com tempos de execuÃ§Ã£o
                - planning_times: Dict com tempos de planejamento
                - generation_times: Dict com tempos de geraÃ§Ã£o
                - errors: Dict com erros (se houver)
                - metadata: EstatÃ­sticas agregadas

        Raises:
            ValueError: Se a query estiver vazia
        """
        if not query or not query.strip():
            raise ValueError("Query nÃ£o pode estar vazia")

        logger.info(f"Comparando respostas (COM ORQUESTRAÃ‡ÃƒO TREE OF THOUGHT) para query: {query[:100]}...")

        manager = self.get_multi_llm()

        result = manager.generate_all_with_orchestration(
            query=query,
            models=models,
            parallel=parallel,
            **kwargs
        )

        logger.info(
            f"ComparaÃ§Ã£o concluÃ­da: {result['metadata']['success_count']} sucessos, "
            f"{result['metadata']['failed_count']} falhas"
        )

        return result

    def compare_and_analyze_full(
        self,
        query: str,
        models: Optional[List[str]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        VersÃ£o mais detalhada da comparaÃ§Ã£o, retornando estatÃ­sticas de tempo
        e anÃ¡lise comparativa entre modelos.

        Args:
            query: Pergunta ou comando
            models: Lista de modelos especÃ­ficos (None = todos)
            **kwargs: ParÃ¢metros extras

        Returns:
            Dict[str, Any]: DicionÃ¡rio com anÃ¡lise comparativa incluindo:
                - query: Query original
                - responses: Respostas dos modelos
                - performance: MÃ©tricas detalhadas por modelo
                - summary: Resumo comparativo
        """
        manager = self.get_multi_llm()
        return manager.compare_responses_with_orchestration(query, models=models, **kwargs)

    def compare_llm_responses(
        self,
        query: str,
        models: Optional[List[str]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        Wrapper simplificado para uso na Interface GrÃ¡fica (UI).
        Sempre executa em modo paralelo para melhor performance.

        Este mÃ©todo Ã© chamado pelo respond() quando o usuÃ¡rio ativa o Multi-LLM na UI.
        Ã‰ uma versÃ£o "amigÃ¡vel" do compare_llm_responses_full() que sempre usa
        as melhores configuraÃ§Ãµes para interface grÃ¡fica.

        Args:
            query: Pergunta a ser processada
            models: Lista de modelos a comparar (None = todos disponÃ­veis)
            **kwargs: ParÃ¢metros extras (temperature, max_tokens, etc)

        Returns:
            Dict[str, Any]: DicionÃ¡rio com estrutura MultiLLMResultFull

        Raises:
            ValueError: Se query estiver vazia ou modelos invÃ¡lidos

        Exemplos:
            >>> agent = openCHA()
            >>> results = agent.compare_llm_responses(
            ...     query="Explique Machine Learning",
            ...     models=["chatgpt", "gemini"]
            ... )
            >>> print(results['responses']['chatgpt'])
            >>> print(results['times']['chatgpt'])
        """
        logger.debug(f"compare_llm_responses() chamado para query: {query[:50]}...")

        return self.compare_llm_responses_full(
            query=query,
            models=models,
            parallel=True,
            **kwargs
        )

    def _run(
        self,
        query: str,
        chat_history: Optional[List[Tuple[str, str]]] = None,
        tasks_list: Optional[List[str]] = None,
        use_history: bool = False,
        **kwargs,
    ) -> str:
        """
        MODO CLÃSSICO (Single Agent):
        Executa a lÃ³gica original do openCHA para um Ãºnico agente.

        âœ… OTIMIZADO: Usa TreeOfThoughtPlanner (Ãºnico disponÃ­vel)

        Args:
            query: Pergunta ou comando
            chat_history: HistÃ³rico da conversa
            tasks_list: Lista de ferramentas/tarefas disponÃ­veis
            use_history: Se True, usa contexto da conversa anterior
            **kwargs: ParÃ¢metros extras

        Returns:
            str: Resposta do agente
        """
        if chat_history is None:
            chat_history = []
        if tasks_list is None:
            tasks_list = []

        history = self._generate_history(chat_history=chat_history)

        if self.orchestrator is None:
            logger.info("Inicializando Orchestrator com TreeOfThoughtPlanner...")
            logger.info("â±ï¸  AVISO: Tree of Thought pode levar 5-30 segundos para responder")
            logger.info("   Isso Ã© NORMAL! O sistema estÃ¡ pensando em mÃºltiplas estratÃ©gias.")

            self.orchestrator = Orchestrator.initialize(
                planner_llm=self.planner_llm,
                planner_name=PlannerType.TREE_OF_THOUGHT,
                datapipe_name=self.datapipe,
                promptist_name=self.promptist,
                response_generator_llm=self.response_generator_llm,
                response_generator_name=self.response_generator,
                available_tasks=tasks_list,
                previous_actions=self.previous_actions,
                verbose=self.verbose,
                **kwargs,
            )
            logger.info("Orchestrator inicializado com sucesso")

        response = self.orchestrator.run(
            query=query,
            meta=self.meta,
            history=history,
            use_history=use_history,
            **kwargs,
        )

        return response

    def run_single_question(self, query: str) -> Tuple[str, float]:
        """
        Roda UMA pergunta com ORQUESTRAÃ‡ÃƒO completa.
        Retorna resposta + tempo em ms.

        Usado pelo benchmark para testar individual cada modelo.

        Args:
            query: A pergunta mÃ©dica

        Returns:
            Tuple[str, float]: (resposta_completa, tempo_em_ms)
        """
        import time
        start = time.time()

        response = self._run(
            query=query,
            chat_history=[],
            tasks_list=[],
            use_history=False
        )

        elapsed = (time.time() - start) * 1000
        return response, elapsed

    def respond(
        self,
        message: str,
        openai_api_key_input: str,
        serp_api_key_input: str,
        gemini_api_key_input: str,
        deepseek_api_key_input: str,
        chat_history: List[Tuple[str, str]],
        check_box: bool,
        tasks_list: List[str],
        use_multi_llm: bool = False,
        compare_models: Optional[List[str]] = None,
    ) -> Tuple[str, List[Tuple[str, str]]]:
        """
        Callback para Interface GrÃ¡fica (UI).
        Recebe as chaves de API da tela e configura o ambiente.
        Agora suporta tanto modo normal quanto Multi-LLM!

        Args:
            message: Mensagem do usuÃ¡rio
            openai_api_key_input: API key da OpenAI
            serp_api_key_input: API key do SERP
            gemini_api_key_input: API key do Gemini
            deepseek_api_key_input: API key do DeepSeek
            chat_history: HistÃ³rico da conversa
            check_box: Flag para usar histÃ³rico
            tasks_list: Lista de tarefas disponÃ­veis
            use_multi_llm: Se True, ativa modo de comparaÃ§Ã£o entre LLMs
            compare_models: Lista de modelos a comparar (ex: ['chatgpt','gemini'])

        Returns:
            Tuple[str, List[Tuple[str, str]]]: Tupla (mensagem_limpa, chat_history_atualizado)
        """
        os.environ["OPENAI_API_KEY"] = openai_api_key_input
        os.environ["SERP_API_KEY"] = serp_api_key_input
        os.environ["GEMINI_API_KEY"] = gemini_api_key_input
        os.environ["DEEPSEEK_API_KEY"] = deepseek_api_key_input

        try:
            if use_multi_llm:
                logger.info("ðŸŒ Respond: modo Multi-LLM ativado")
                logger.info(f"Modelos selecionados: {compare_models}")

                results = self.compare_llm_responses(
                    query=message,
                    models=compare_models if compare_models else None,
                )

                response = self._format_multi_llm_results(results)

            else:
                logger.info("ðŸ¤– Respond: modo Normal (single agent) com TreeOfThought")
                response = self._run(
                    query=message,
                    chat_history=chat_history,
                    tasks_list=tasks_list,
                    use_history=check_box,
                )

                # ðŸ” DEBUG
                print(f"\n{'='*60}")
                print(f"ðŸ” DEBUG - Query: {message}")
                print(f"Response (primeiros 200 chars): {response[:200]}")
                print(f"Tem 'Desculpe'?: {'Desculpe' in response}")
                print(f"{'='*60}\n")

            files = parse_addresses(response)

            if len(files) == 0:
                chat_history.append((message, response))
            else:
                for i in range(len(files)):
                    chat_history.append(
                        (
                            message if i == 0 else None,
                            response[: files[i][1]],
                        )
                    )
                    chat_history.append((None, (files[i][0],)))
                    response = response[files[i][2] :]

            return "", chat_history

        except Exception as e:
            error_msg = f"Erro ao processar mensagem: {str(e)}"
            logger.error(error_msg, exc_info=True)
            chat_history.append((message, f"âŒ {error_msg}"))
            return "", chat_history

    def reset(self) -> None:
        """
        Limpa tudo para comeÃ§ar do zero.
        Reseta o orchestrator, histÃ³rico de aÃ§Ãµes e cache do Multi-LLM.
        """
        logger.info("Resetando estado do openCHA...")
        self.previous_actions = []
        self.meta = []
        self.orchestrator = None

        if self.multi_llm is not None:
            self.multi_llm.clear_cache()

        logger.info("Estado resetado com sucesso")

    def run_with_interface(self) -> None:
        """
        LanÃ§a a interface visual (Gradio).
        Configura todos os callbacks e inicia o servidor web.
        """
        logger.info("Iniciando interface grÃ¡fica...")
        available_tasks = [key.value for key in TASK_TO_CLASS.keys()]
        interface = Interface()
        interface.prepare_interface(
            respond=self.respond,
            reset=self.reset,
            upload_meta=self.upload_meta,
            available_tasks=available_tasks,
        )

    def upload_meta(self, history: List[Tuple], file: Any) -> List[Tuple]:
        """
        Lida com upload de arquivos na UI.

        Args:
            history: HistÃ³rico atual do chat
            file: Arquivo enviado pelo usuÃ¡rio

        Returns:
            List[Tuple]: HistÃ³rico atualizado com o arquivo
        """
        history = history + [((file.name,), None)]
        self.meta.append(file.name)
        logger.info(f"Arquivo uploaded: {file.name}")
        return history

    def run(
        self,
        query: str,
        chat_history: Optional[List[Tuple[str, str]]] = None,
        available_tasks: Optional[List[str]] = None,
        use_history: bool = False,
        use_multi_llm: bool = False,
        compare_models: Optional[List[str]] = None,
        **kwargs,
    ) -> str:
        """
        O NOVO PONTO DE ENTRADA PRINCIPAL.
        Decide se roda o modo normal ou o modo de comparaÃ§Ã£o (Multi-LLM).

        âœ… OTIMIZADO: Usa TreeOfThoughtPlanner com timeout aumentado

        Args:
            query: Pergunta ou comando
            chat_history: HistÃ³rico da conversa
            available_tasks: Lista de tarefas/ferramentas disponÃ­veis
            use_history: Se True, usa contexto anterior
            use_multi_llm: Se True, ativa comparaÃ§Ã£o entre mÃºltiplos modelos
            compare_models: Lista de modelos a comparar
            **kwargs: ParÃ¢metros extras

        Returns:
            str: Resposta formatada (texto simples para modo normal,
                 relatÃ³rio comparativo para Multi-LLM)

        Examples:
            >>> # Modo Normal
            >>> agent = openCHA()
            >>> response = agent.run(
            ...     query="Explique IA",
            ...     use_multi_llm=False
            ... )
            >>>
            >>> # Modo Multi-LLM
            >>> response = agent.run(
            ...     query="Explique IA",
            ...     use_multi_llm=True,
            ...     compare_models=["chatgpt", "gemini"]
            ... )
        """
        if chat_history is None:
            chat_history = []
        if available_tasks is None:
            available_tasks = []

        try:
            if use_multi_llm:
                logger.info("ðŸŒ Executando em MODO COMPARAÃ‡ÃƒO COM ORQUESTRAÃ‡ÃƒO TREE OF THOUGHT")
                logger.info(f"Modelos: {compare_models if compare_models else 'todos disponÃ­veis'}")

                results = self.compare_llm_responses_full(
                    query,
                    models=compare_models,
                    **kwargs
                )
                return self._format_multi_llm_results(results)

            else:
                logger.info("ðŸ¤– Executando em MODO NORMAL (single agent com TreeOfThought)")
                return self._run(
                    query=query,
                    chat_history=chat_history,
                    tasks_list=available_tasks,
                    use_history=use_history,
                    **kwargs,
                )

        except Exception as e:
            error_msg = f"Erro ao executar query: {str(e)}"
            logger.error(error_msg, exc_info=True)
            return f"âŒ {error_msg}"

    def _format_multi_llm_results(self, results: Dict[str, Any]) -> str:
        """
        Transforma o JSON de resultados em um relatÃ³rio de texto legÃ­vel.
        Exibe tempos de planejamento e execuÃ§Ã£o separadamente.

        Args:
            results: DicionÃ¡rio com estrutura MultiLLMResultFull

        Returns:
            str: RelatÃ³rio formatado em texto
        """
        output_lines = [
            "=" * 80,
            "COMPARAÃ‡ÃƒO ENTRE MÃšLTIPLOS LLMs (COM ORQUESTRAÃ‡ÃƒO TREE OF THOUGHT)",
            "=" * 80,
            ""
        ]

        metadata = results['metadata']
        output_lines.extend([
            f"â±ï¸  Tempo total: {metadata['total_time_ms']} ms",
            f"âœ… Sucessos: {metadata['success_count']} | âŒ Falhas: {metadata['failed_count']}",
            f"ðŸ”¤ Tokens estimados: {metadata['total_tokens_estimate']}",
            f"ðŸ§  Tipo de execuÃ§Ã£o: {metadata['execution_type']}",
            ""
        ])

        for model_name, response in results['responses'].items():
            time_ms = results['times'][model_name]
            planning_time = results['planning_times'][model_name]
            generation_time = results['generation_times'][model_name]
            error = results['errors'][model_name]

            output_lines.extend([
                f"{'=' * 80}",
                f"ðŸ¤– {model_name.upper()}",
                f"{'=' * 80}",
            ])

            if error:
                output_lines.append(f"âŒ Erro: {error}")
            else:
                output_lines.extend([
                    f"â±ï¸  Tempo total: {time_ms} ms",
                    f"  â”œâ”€ ðŸ§  Planejamento: {planning_time:.1f} ms",
                    f"  â””â”€ âœï¸  GeraÃ§Ã£o: {generation_time:.1f} ms",
                    f"ðŸ“ Resposta:",
                    f"{response}",
                ])

            output_lines.append("")

        valid_times = {k: v for k, v in results['times'].items() if v is not None}
        if valid_times:
            fastest = min(valid_times.items(), key=lambda x: x[1])
            output_lines.extend([
                f"{'=' * 80}",
                f"ðŸ† Modelo mais rÃ¡pido: {fastest[0].upper()} ({fastest[1]} ms)",
                f"{'=' * 80}",
            ])

        return "\n".join(output_lines)

    def get_available_models(self) -> List[str]:
        """
        Helper para saber quais modelos estÃ£o disponÃ­veis e funcionando.

        Returns:
            List[str]: Lista de nomes dos modelos disponÃ­veis
        """
        manager = self.get_multi_llm()
        return manager.get_available_models()

    def clear_multi_llm_cache(self) -> None:
        """
        Limpa cache especificamente do MultiLLM.
        Ãštil quando vocÃª quer respostas frescas mesmo para queries repetidas.
        """
        if self.multi_llm is not None:
            self.multi_llm.clear_cache()
            logger.info("Cache do MultiLLMManager limpo")
        else:
            logger.warning("MultiLLMManager nÃ£o foi inicializado ainda")
