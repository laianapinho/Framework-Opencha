#!/usr/bin/env python3
"""
Script de teste para verificar se ChatGPT, Gemini e DeepSeek funcionam
Execute no terminal para diagnosticar o problema
"""

import os
import sys

print("=" * 80)
print("ğŸ§ª TESTE DE LLMs - Verificar se funcionam")
print("=" * 80)

# TESTE 1: Verificar se API keys existem
print("\n1ï¸âƒ£ VERIFICANDO API KEYS...")
print("-" * 80)

openai_key = os.environ.get("OPENAI_API_KEY")
gemini_key = os.environ.get("GEMINI_API_KEY")
deepseek_key = os.environ.get("DEEPSEEK_API_KEY")

print(f"OPENAI_API_KEY: {openai_key[:10]}..." if openai_key else "OPENAI_API_KEY: âŒ NÃƒO CONFIGURADA")
print(f"GEMINI_API_KEY: {gemini_key[:10]}..." if gemini_key else "GEMINI_API_KEY: âŒ NÃƒO CONFIGURADA")
print(f"DEEPSEEK_API_KEY: {deepseek_key[:10]}..." if deepseek_key else "DEEPSEEK_API_KEY: âŒ NÃƒO CONFIGURADA")

if not openai_key or not deepseek_key:
    print("\nâŒ ERRO: API keys nÃ£o estÃ£o configuradas!")
    print("\nConfigure assim:")
    print('  export OPENAI_API_KEY="sua-chave-aqui"')
    print('  export DEEPSEEK_API_KEY="sua-chave-aqui"')
    print('  export GEMINI_API_KEY="sua-chave-aqui"')
    sys.exit(1)

print("âœ… API keys encontradas!")

# TESTE 2: Importar as classes
print("\n2ï¸âƒ£ IMPORTANDO CLASSES...")
print("-" * 80)

try:
    from openCHA.llms import initialize_llm, LLMType
    print("âœ… ImportaÃ§Ã£o bem-sucedida")
except Exception as e:
    print(f"âŒ Erro ao importar: {e}")
    sys.exit(1)

# TESTE 3: Testar cada modelo individualmente
print("\n3ï¸âƒ£ TESTANDO CADA MODELO...")
print("-" * 80)

modelos = {
    'chatgpt': LLMType.OPENAI,
    'deepseek': LLMType.DEEPSEEK,
    'gemini': LLMType.GEMINI,
}

test_query = "What are the main symptoms of cancer?"
resultados = {}

for nome, llm_type in modelos.items():
    print(f"\nâ–¶ï¸  Testando {nome.upper()}...")

    try:
        # ETAPA 1: Criar instÃ¢ncia
        print(f"   â”œâ”€ Criando instÃ¢ncia...", end=" ")
        llm = initialize_llm(llm_type)
        print("âœ…")

        # ETAPA 2: Testar geraÃ§Ã£o
        print(f"   â”œâ”€ Gerando resposta...", end=" ")
        response = llm.generate(
            test_query,
            max_tokens=50,
            temperature=0
        )
        print("âœ…")

        # ETAPA 3: Validar resposta
        print(f"   â”œâ”€ Validando resposta...", end=" ")
        if response and isinstance(response, str) and len(response.strip()) > 5:
            print("âœ…")
            print(f"   â””â”€ ğŸ“ Resposta: {response[:80]}...")
            resultados[nome] = "âœ… FUNCIONANDO"
        else:
            print("âŒ")
            print(f"   â””â”€ ğŸ“ Resposta vazia ou invÃ¡lida: {response!r}")
            resultados[nome] = "âš ï¸ RESPOSTA VAZIA"

    except Exception as e:
        print("âŒ")
        error_type = type(e).__name__
        error_msg = str(e)
        print(f"   â””â”€ âŒ {error_type}: {error_msg}")
        resultados[nome] = f"âŒ {error_type}"

# TESTE 4: Resumo
print("\n" + "=" * 80)
print("4ï¸âƒ£ RESUMO DOS RESULTADOS")
print("=" * 80)

for nome, status in resultados.items():
    print(f"{nome.upper():<15} {status}")

print("\n" + "=" * 80)

# AnÃ¡lise
funcionando = sum(1 for s in resultados.values() if "âœ…" in s)
total = len(resultados)

print(f"\nğŸ“Š {funcionando}/{total} modelos funcionando")

if funcionando == 0:
    print("\nâŒ PROBLEMA: Nenhum modelo funciona!")
    print("\nPossÃ­veis causas:")
    print("  1. API keys invÃ¡lidas")
    print("  2. Problema de conexÃ£o de rede")
    print("  3. Quota excedida")
    print("  4. Firewall bloqueando")
elif funcionando < total:
    print(f"\nâš ï¸ AVISO: Apenas {funcionando} modelo(s) funcionando")
    print("   Os outros precisam ser investigados")
else:
    print("\nâœ… SUCESSO: Todos os modelos funcionam!")

print("\n" + "=" * 80)
